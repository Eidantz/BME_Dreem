{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014251,
     "end_time": "2021-01-04T15:50:28.482832",
     "exception": false,
     "start_time": "2021-01-04T15:50:28.468581",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dreem 2 Sleep Classification challenge 2020\n",
    "**Student: Felipe Cybis Pereira**\n",
    "\n",
    "This notebook is the main code for training and producing test data for the challenge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-04T15:50:28.518797Z",
     "iopub.status.busy": "2021-01-04T15:50:28.518081Z",
     "iopub.status.idle": "2021-01-04T15:50:28.720642Z",
     "shell.execute_reply": "2021-01-04T15:50:28.719778Z"
    },
    "papermill": {
     "duration": 0.224793,
     "end_time": "2021-01-04T15:50:28.720756",
     "exception": false,
     "start_time": "2021-01-04T15:50:28.495963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dreem-2-sleep-classification-challenge-2020/sample_submission.csv\n",
      "/kaggle/input/dreem-2-sleep-classification-challenge-2020/y_train.csv\n",
      "/kaggle/input/dreem-2-sleep-classification-challenge-2020/X_train.h5/X_train.h5\n",
      "/kaggle/input/dreem-2-sleep-classification-challenge-2020/X_test.h5/X_test.h5\n",
      "/kaggle/input/notebook-producing-spectrograms/Sxx_x_train.h5\n",
      "/kaggle/input/notebook-producing-spectrograms/y_train.csv\n",
      "/kaggle/input/notebook-producing-spectrograms/__results__.html\n",
      "/kaggle/input/notebook-producing-spectrograms/Sxx_x_test.h5\n",
      "/kaggle/input/notebook-producing-spectrograms/__notebook__.ipynb\n",
      "/kaggle/input/notebook-producing-spectrograms/__output__.json\n",
      "/kaggle/input/notebook-producing-spectrograms/custom.css\n",
      "['Sxx_1', 'Sxx_2', 'Sxx_3', 'Sxx_4', 'Sxx_5', 'Sxx_6', 'Sxx_7', 'index', 'index_window']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import h5py # Read and write HDF5 files from Python\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "# filenames for data we will work with\n",
    "data_path = \"/kaggle/input/notebook-producing-spectrograms/\"\n",
    "file_xtrain = data_path + \"Sxx_x_train.h5\"\n",
    "file_xtest = data_path + \"Sxx_x_test.h5\"\n",
    "file_ytrain = data_path + \"y_train.csv\"\n",
    "\n",
    "data_path_eeg = \"/kaggle/input/dreem-2-sleep-classification-challenge-2020/\"\n",
    "file_eeg_xtrain = data_path_eeg + \"X_train.h5/X_train.h5\"\n",
    "\n",
    "# training labels\n",
    "y_data = pd.read_csv(file_ytrain)\n",
    "\n",
    "# what does the h5 file contains ?\n",
    "with h5py.File(file_xtrain, \"r\") as hf:\n",
    "    fields = list(hf.keys())\n",
    "    print(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013099,
     "end_time": "2021-01-04T15:50:28.747752",
     "exception": false,
     "start_time": "2021-01-04T15:50:28.734653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We are going to use the spectrogram data produced by `notebook-producing-spectrograms`. Note that instead of the 'eeg_i' fields we have 'Sxx_i' and the same 'index' and 'index_window' fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-01-04T15:50:28.792555Z",
     "iopub.status.busy": "2021-01-04T15:50:28.787285Z",
     "iopub.status.idle": "2021-01-04T15:50:30.176031Z",
     "shell.execute_reply": "2021-01-04T15:50:30.174822Z"
    },
    "papermill": {
     "duration": 1.415043,
     "end_time": "2021-01-04T15:50:30.176161",
     "exception": false,
     "start_time": "2021-01-04T15:50:28.761118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def get_train_validation_dataset(channel_list, ratio=0.2, k_neighbor=0):\n",
    "    \"\"\"\n",
    "    Get train and validation set by splitting the training set. File path for x_data and for y_data are hard coded\n",
    "    within the function.\n",
    "        :param channnel_list (list): name of channels to use from the spectrograms available\n",
    "        :param ratio (default=0.2): ratio to split into training set (1-ratio) and validation set (ratio)\n",
    "        The splitting is done randomly and subject-wise!\n",
    "        :param k_neighbor (int): number of epochs to append before and after the main epoch\n",
    "        Number of epochs to be returned by the data set is (2*k_neighbor+1)\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Channel being used: ' + str(channel_list))\n",
    "    with h5py.File(file_xtrain, \"r\") as fi:\n",
    "        x_data = np.stack(list(fi[channel][()] for channel in channel_list), axis=0)\n",
    "        index_window = fi['index_window'][()]\n",
    "        index = fi['index'][()]\n",
    "    print('Input data shape: ' + str(x_data.shape))\n",
    "    y_data = pd.read_csv(file_ytrain)['sleep_stage'].to_numpy()\n",
    "    \n",
    "    subject_idx = np.unique(index)\n",
    "    split = int((1-ratio) * len(subject_idx))\n",
    "    \n",
    "    # shuffling subjects\n",
    "    np.random.shuffle(subject_idx)\n",
    "    train_subjects, val_subjects = subject_idx[:split], subject_idx[split:]\n",
    "    print('Training subjects: ' + str(train_subjects))\n",
    "    print('Validation subjects: ' + str(val_subjects))\n",
    "    \n",
    "    train_indices = np.where(np.in1d(index, train_subjects))[0]\n",
    "    val_indices = np.where(np.in1d(index, val_subjects))[0]\n",
    "    \n",
    "    x_train, x_validation = x_data[:,train_indices], x_data[:,val_indices]\n",
    "    y_train, y_validation = y_data[train_indices], y_data[val_indices]\n",
    "    idx_window_train, idx_window_validation = index_window[train_indices], index_window[val_indices]\n",
    "    idx_train, idx_validation = index[train_indices], index[val_indices]\n",
    "    \n",
    "    train_ds = SxxEpochDataset(x_data=x_train,\n",
    "                               y_data=y_train,\n",
    "                               index_window=idx_window_train,\n",
    "                               k_neighbor=k_neighbor)\n",
    "    \n",
    "    val_ds = SxxEpochDataset(x_data=x_validation,\n",
    "                             y_data=y_validation,\n",
    "                             index_window=idx_window_validation,\n",
    "                             k_neighbor=k_neighbor)\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "\n",
    "def get_test_dataset(channel_list, k_neighbor=0):\n",
    "    \"\"\"\n",
    "    Get test set. File path for x_data is hard coded within the function.\n",
    "        :param channnel_list (list): name of channels to use from the spectrograms available\n",
    "        :param k_neighbor (int): number of epochs to append before and after the main epoch\n",
    "        Number of epochs to be returned by the data set is (2*k_neighbor+1)\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Channel being used: ' + str(channel_list))\n",
    "    with h5py.File(file_xtest, \"r\") as fi:\n",
    "        x_test = np.stack(list(fi[channel][()] for channel in channel_list), axis=0)\n",
    "        index_window = fi['index_window'][()]\n",
    "        index = fi['index'][()]\n",
    "    print('Input data shape: ' + str(x_test.shape))\n",
    "    subjects = np.unique(index)\n",
    "    print('Test subjects: ' + str(subjects))\n",
    "    \n",
    "    \n",
    "    test_ds = SxxEpochDataset(x_data=x_test,\n",
    "                              y_data=None,\n",
    "                              index_window=index_window,\n",
    "                              k_neighbor=k_neighbor)\n",
    "\n",
    "    \n",
    "    return test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T15:50:30.229387Z",
     "iopub.status.busy": "2021-01-04T15:50:30.222968Z",
     "iopub.status.idle": "2021-01-04T15:50:30.232179Z",
     "shell.execute_reply": "2021-01-04T15:50:30.231663Z"
    },
    "papermill": {
     "duration": 0.042028,
     "end_time": "2021-01-04T15:50:30.232286",
     "exception": false,
     "start_time": "2021-01-04T15:50:30.190258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SxxEpochDataset(Dataset):\n",
    "    \"\"\"Create PyTorch dataset for spectrogram epochs.\"\"\"\n",
    "\n",
    "    def __init__(self, x_data, y_data=None, index_window=None, k_neighbor=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_data (numpy array): Numpy array of input data.\n",
    "            y_data (list of numpy array): Sleep Stages\n",
    "            index_window (numpy array): Index of epoch in a particular subject\n",
    "            k_neighbor (int or float): Number of epochs to be concatenated before and after the main epoch.\n",
    "            Total number of epochs is (2*k_neighbor+1)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.y_data = y_data\n",
    "        if self.y_data is None:\n",
    "            print('Labels not given, dataset will return only x values')\n",
    "        self.index_window = index_window\n",
    "        assert self.index_window is not None, 'Intra subject epoch indexing is needed for the right padding'\n",
    "        \n",
    "        self.x_data = x_data\n",
    "        self.num_channels = self.x_data.shape[0]\n",
    "        self.data_size = self.x_data.shape[1]\n",
    "        self.freq_size = self.x_data.shape[2]\n",
    "        self.one_epoch_length = self.x_data.shape[3]\n",
    "        \n",
    "        self.k_neighbor = k_neighbor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        main_epoch_id = self.index_window[idx]\n",
    "        \n",
    "        if idx >= self.k_neighbor and idx <= self.data_size - (self.k_neighbor+1):\n",
    "            # idx is in the \"safe zone\" (almost always)\n",
    "            indices = self.index_window[idx - self.k_neighbor: idx + self.k_neighbor + 1]\n",
    "            epochs = np.copy(self.x_data[:,idx - self.k_neighbor: idx + self.k_neighbor + 1])\n",
    "            \n",
    "            differences_to_check = np.arange(main_epoch_id - self.k_neighbor, main_epoch_id + self.k_neighbor + 1)\n",
    "            # checking if indices are all good or if needs padding\n",
    "            check_indices = np.equal(indices,differences_to_check)\n",
    "            \n",
    "            # This padding is easier, if there is some gap in self.index_window the data or if data is changing\n",
    "            # subjects in the sequence, we can just multiply the bad epochs by zero\n",
    "            if not np.all(check_indices):\n",
    "                for ii, checked_index in enumerate(check_indices):\n",
    "                    if not checked_index:\n",
    "                        epochs[:,ii] = epochs[:,ii]*0 \n",
    "        \n",
    "        # If idx is lower than k_neighbors there is not data to fetch before the beggining\n",
    "        # of the dataset. We must create zero padding for it\n",
    "        elif idx < self.k_neighbor:\n",
    "            nb_epochs_to_pad = self.k_neighbor - idx\n",
    "            epochs_to_pad = np.zeros((self.num_channels, nb_epochs_to_pad, self.freq_size, self.one_epoch_length))\n",
    "            epochs = np.copy(self.x_data[:,:idx + self.k_neighbor + 1])\n",
    "            \n",
    "            epochs = np.concatenate((epochs_to_pad, epochs), axis=1)\n",
    "            \n",
    "        # Same thing if idx is too high\n",
    "        elif idx > self.data_size - (self.k_neighbor+1):\n",
    "            nb_epochs_to_pad = idx - (self.data_size - (self.k_neighbor+1))\n",
    "            epochs_to_pad = np.zeros((self.num_channels, nb_epochs_to_pad, self.freq_size, self.one_epoch_length))\n",
    "            epochs = np.copy(self.x_data[:,idx - self.k_neighbor:])\n",
    "            \n",
    "            epochs = np.concatenate((epochs, epochs_to_pad), axis=1)\n",
    "        \n",
    "            \n",
    "        epochs = epochs.swapaxes(2,1).reshape(self.num_channels,self.freq_size,-1)\n",
    "        \n",
    "        if self.y_data is not None:\n",
    "            label = self.y_data[idx]\n",
    "            return epochs.astype('float32'), label\n",
    "\n",
    "        return epochs.astype('float32')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T15:50:30.283233Z",
     "iopub.status.busy": "2021-01-04T15:50:30.281555Z",
     "iopub.status.idle": "2021-01-04T15:50:30.284010Z",
     "shell.execute_reply": "2021-01-04T15:50:30.284482Z"
    },
    "papermill": {
     "duration": 0.037735,
     "end_time": "2021-01-04T15:50:30.284593",
     "exception": false,
     "start_time": "2021-01-04T15:50:30.246858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\"\"\"\n",
    "Bottleneck Attention Module (BAM) such as in Park et al. (arXiv:1807.06514)\n",
    "Inspired by https://medium.com/visionwizard/understanding-attention-modules-cbam-and-bam-a-quick-read-ca8678d1c671\n",
    "\"\"\"\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "        \n",
    "        \n",
    "class ChannelAttentionGate(nn.Module):\n",
    "    \"\"\"Channel attention by global average pooling, then encoding and decoding linear layers\n",
    "    with a reduction rate of (default=16)\"\"\"\n",
    "    def __init__(self, num_channels, reduction_ratio=16):\n",
    "        super().__init__()\n",
    "  \n",
    "        self.gate_c = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            Flatten(),\n",
    "            nn.Linear(num_channels, num_channels // reduction_ratio),\n",
    "            nn.BatchNorm1d(num_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_channels // reduction_ratio, num_channels),\n",
    "            nn.BatchNorm1d(num_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        # unsqueezing and expanding so it has same dimension as input\n",
    "        xb = self.gate_c(xb).unsqueeze(2).unsqueeze(3).expand_as(xb)\n",
    "        return xb\n",
    "        \n",
    "class SpatialAttentionGate(nn.Module):\n",
    "    \"\"\"Spatial attention then encoding and decoding convolutional layers\n",
    "    with a reduction rate of (default=16)\"\"\"\n",
    "    def __init__(self, num_channels, reduction_ratio=16, dilation_val=4):\n",
    "        super().__init__()\n",
    "        self.gate_s = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, num_channels // reduction_ratio, kernel_size=1),\n",
    "            nn.BatchNorm2d(num_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_channels // reduction_ratio, num_channels // reduction_ratio, kernel_size=3, padding=dilation_val, dilation=dilation_val),\n",
    "            nn.BatchNorm2d(num_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_channels // reduction_ratio, num_channels // reduction_ratio, kernel_size=3, padding=dilation_val, dilation=dilation_val),\n",
    "            nn.BatchNorm2d(num_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_channels // reduction_ratio, 1, kernel_size=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        # expanding so it has same dimension as input\n",
    "        xb = self.gate_s(xb).expand_as(xb)\n",
    "        return xb\n",
    "\n",
    "class BAM(nn.Module):\n",
    "    \"\"\"Bottleneck Attention Module, puts together\n",
    "    SpatialAttentionGate and ChannelAttentionGate, with a residual learning scheme.\"\"\"\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "        self.channel_attention = ChannelAttentionGate(num_channels=num_channels)\n",
    "        self.spatial_attention = SpatialAttentionGate(num_channels=num_channels)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        attention = torch.sigmoid(self.channel_attention(xb) + self.spatial_attention(xb))\n",
    "        return xb + xb*attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T15:50:30.338560Z",
     "iopub.status.busy": "2021-01-04T15:50:30.336775Z",
     "iopub.status.idle": "2021-01-04T15:50:30.339251Z",
     "shell.execute_reply": "2021-01-04T15:50:30.339716Z"
    },
    "papermill": {
     "duration": 0.041027,
     "end_time": "2021-01-04T15:50:30.339824",
     "exception": false,
     "start_time": "2021-01-04T15:50:30.298797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\"\"\" \n",
    "Creating PyTorch module for the Convolutional Neural Network with optional attention module and k_neighboring epochs.\n",
    "One spectrogram epoch has dimensions (40,60).\n",
    "Input dimensions is (40, (2*k_neighbor+1)*60).\n",
    "There are 3 max_pool2d along the net during the convolutional blocks, kernels being (2,4), (2,3), (2,5).\n",
    "-> This means that before the fully connected blocks, the data has dimensions (5, (2*k_neighbor+1)).\n",
    "\n",
    "First block:\n",
    "        - BatchNorm2d\n",
    "        - Conv2d(input=channels, output=64)\n",
    "        - ReLU\n",
    "        - Bottleneck Attention Module (optional)\n",
    "        - MaxPool2d\n",
    "        \n",
    "Main blocks structure: \n",
    "        - BatchNorm2d\n",
    "        - Conv2d(input=input, output=input*2)\n",
    "        - ReLU\n",
    "        - Conv2d(input=input*2, output=input*4)\n",
    "        - ReLU\n",
    "        - Bottleneck Attention Module (optional)\n",
    "        - MaxPool2d\n",
    "        \n",
    "Final fully connected blocks:\n",
    "        - Dropout (50%)\n",
    "        - Linear\n",
    "        - LeakyReLU (-0.1)\n",
    "        - Dropout (50%)\n",
    "        - Linear\n",
    "\n",
    "Two fully connected layers with dropout(50%) before each one.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class Dreem_CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channels,\n",
    "                 k_neighbors=0,\n",
    "                 use_attention=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # number of occipital and frontal channels\n",
    "        self.num_channels = channels\n",
    "        \n",
    "        \n",
    "        self.k_neighbors = k_neighbors ## 40, (2*k+1)*60\n",
    "        self.activ_relu = nn.ReLU()\n",
    "        self.activ_leakyrelu = nn.LeakyReLU(negative_slope=0.1)\n",
    "\n",
    "\n",
    "        self.conv_bloc_1 = nn.Sequential()\n",
    "        self.conv_bloc_1.add_module('init_bn', nn.BatchNorm2d(self.num_channels))\n",
    "        self.conv_bloc_1.add_module('init_conv2d', nn.Conv2d(self.num_channels, 64, kernel_size=(3,3), stride=1, padding=(1,1)))\n",
    "        self.conv_bloc_1.add_module('init_activ', self.activ_relu)\n",
    "        if use_attention:\n",
    "            self.conv_bloc_1.add_module('init_bam', BAM(64))\n",
    "        self.conv_bloc_1.add_module('init_maxpool2d', nn.MaxPool2d(kernel_size=(2,4)))\n",
    "\n",
    "        self.conv_bloc_2 = self._make_conv_bloc(init_input=64, max_pool_kernel=(2,3), with_attention=use_attention)\n",
    "\n",
    "        self.conv_bloc_3 = self._make_conv_bloc(init_input=256, max_pool_kernel=(2,5), with_attention=use_attention)\n",
    "        \n",
    "        ## need reshape before entering here\n",
    "        self.full_conn_bloc = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(int(1024*5*(2*self.k_neighbors+1)),100),\n",
    "            self.activ_leakyrelu,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.classification_conn_bloc = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(100,5)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def _make_conv_bloc(self, init_input, max_pool_kernel, with_attention=False):\n",
    "        conv_bloc = nn.Sequential()\n",
    "        conv_bloc.add_module('bn2d', nn.BatchNorm2d(init_input))\n",
    "        conv_bloc.add_module('conv2d_1', nn.Conv2d(init_input, 2*init_input, kernel_size=3, stride=1, padding=(1,1)))\n",
    "        conv_bloc.add_module('activ', self.activ_relu)\n",
    "        conv_bloc.add_module('conv2d_2', nn.Conv2d(2*init_input, 2*2*init_input, kernel_size=3, stride=1, padding=(1,1)))\n",
    "        conv_bloc.add_module('activ', self.activ_relu)\n",
    "        if with_attention:\n",
    "            conv_bloc.add_module('attention', BAM(2*2*init_input))\n",
    "        conv_bloc.add_module('maxpool2d', nn.MaxPool2d(kernel_size=max_pool_kernel))\n",
    "        \n",
    "        return conv_bloc\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        \n",
    "        xb = self.conv_bloc_1(xb)\n",
    "        xb = self.conv_bloc_2(xb)\n",
    "        xb = self.conv_bloc_3(xb)\n",
    "        \n",
    "        xb = xb.reshape(xb.shape[0],-1) # flatten all dimensions except batch dimension\n",
    "        xb = self.full_conn_bloc(xb)\n",
    "        xb = self.classification_conn_bloc(xb)\n",
    "\n",
    "        return xb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T15:50:30.383895Z",
     "iopub.status.busy": "2021-01-04T15:50:30.375683Z",
     "iopub.status.idle": "2021-01-04T15:50:31.198170Z",
     "shell.execute_reply": "2021-01-04T15:50:31.196754Z"
    },
    "papermill": {
     "duration": 0.844463,
     "end_time": "2021-01-04T15:50:31.198326",
     "exception": false,
     "start_time": "2021-01-04T15:50:30.353863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score, confusion_matrix, f1_score\n",
    "\"\"\"\n",
    "Useful functions to be used in the training function.\n",
    "\"\"\"\n",
    "\n",
    "def loss_val(net, val_loader, criterion):\n",
    "    \"\"\"\n",
    "    Function to compute validation loss (not computed during retropropagation!)\n",
    "        :param net: pytorch model\n",
    "        :param val_loader: pytorch dataloader\n",
    "        :param criterion: loss function\n",
    "        \n",
    "        :return val_loss: mean loss in whole dataset\n",
    "    \"\"\"\n",
    "    with torch.no_grad(): # do not forget to remove gradient computing during evaluation !!!\n",
    "        val_loss = []\n",
    "        net.eval()\n",
    "        for inputs, labels in val_loader:\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += [loss.item()]\n",
    "            \n",
    "    net.train()\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "def evaluate_scores(net, dataloader):\n",
    "    \"\"\"\n",
    "    To evaluate validation scores. Uses the evaluate function just below\n",
    "        :param net: pytorch network model\n",
    "        :param dataloader: pytorch dataloader\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        prediction_list = torch.empty(0).to(device)\n",
    "        true_list = torch.empty(0).to(device)\n",
    "\n",
    "        net.eval() # set net to evaluation mode (necessary if using batch normalization, for example)\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            prediction_list = torch.cat([prediction_list, predicted])\n",
    "            true_list = torch.cat([true_list, labels])\n",
    "\n",
    "        # Scores\n",
    "        true_list = true_list.cpu().numpy()\n",
    "        prediction_list = prediction_list.cpu().numpy()\n",
    "        scores = evaluate(true_list, prediction_list)\n",
    "        \n",
    "    net.train() # reset net to training mode to continue training\n",
    "    return scores \n",
    "    \n",
    "    \n",
    "# score function\n",
    "def evaluate(true, pred):\n",
    "    \"\"\"\n",
    "    Function using sklearn.metrics functions to score predicted labels against true labels.\n",
    "    f1_score with weighted average is the scoring used in the challenge!\n",
    "    \"\"\"\n",
    "    scores = {'balanced_accuracy': balanced_accuracy_score(true, pred),\n",
    "            'cohen_kappa': cohen_kappa_score(true, pred),\n",
    "            'confusion_matrix': confusion_matrix(true, pred),\n",
    "             'mean_f1_score': f1_score(true,pred,average='weighted')}\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T15:50:31.258372Z",
     "iopub.status.busy": "2021-01-04T15:50:31.237001Z",
     "iopub.status.idle": "2021-01-04T15:50:31.279893Z",
     "shell.execute_reply": "2021-01-04T15:50:31.279403Z"
    },
    "papermill": {
     "duration": 0.065608,
     "end_time": "2021-01-04T15:50:31.280027",
     "exception": false,
     "start_time": "2021-01-04T15:50:31.214419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data.dataset import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "def train_model(model, \n",
    "                batch_size=64,\n",
    "                n_epoch=15,\n",
    "                k_fold=3,\n",
    "                loss_func=nn.CrossEntropyLoss,\n",
    "                optimizer=optim.AdamW,\n",
    "                learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Function to train the model with k-fold cross validation.\n",
    "        :param model: PyTorch model\n",
    "        :param batch_size: for batch training, usually set to 32, 62, 128... (default=64)\n",
    "        :param n_epoch: number of epochs to train (default=15)\n",
    "        :param k-fold: number of k-fold cross validation (default=3).\n",
    "        It will subdivide the training dataset in 3 subsets and train model in 2 of them while validating on the third one.\n",
    "        For each epoch, 3 iterations is done (train:(0,1) validate:(2), train:(1,2) validate:(0), train:(0,2) validate:(1))\n",
    "        :param loss_func: loss function to be used. If LogSoftmax function is used in the model, we suggest to use NLLLoss.\n",
    "        If not, CrossEntropyLoss is suggested.\n",
    "        :param optimizer: optimizer method to be used (default=AdamW (to be used with weight_decay))\n",
    "        :param learning_rate: starting learning rate (default=0.001)\n",
    "\n",
    "        :return best_model: the best model according to the mean validation loss (list)\n",
    "        :return all_val_loss: mean validation loss for each epoch\n",
    "        :return k_fold_loss: validation loss for each k-fold for each epoch\n",
    "        :return val_scores: dictionaire with scores for each k-fold for each epoch\n",
    "        :return fin_val_loss: validation loss for each epoch in unseen data\n",
    "        :return fin_val_scores: dictionaire with scores for each epoch in unseen data\n",
    "\n",
    "    Scores used are the following sklearn.metrics functions:\n",
    "        [balanced_accuracy_score, cohen_kappa_score, confusion_matrix, f1_score]\n",
    "        \n",
    "    OBS: This is a tunable function. It is highly recommended to go beyond the parameters mentioned above.\n",
    "    It is not a \"plug-and-play\" function, it should be adapted to the case of study!!\n",
    "    \"\"\"\n",
    "\n",
    "    print('Using GPU:', torch.cuda.is_available())\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    bs = batch_size\n",
    "    n_epoch = n_epoch\n",
    "    k_fold = k_fold\n",
    "  \n",
    "    criterion = loss_func()\n",
    "    new_learning_rate = learning_rate\n",
    "    optim = optimizer(model.parameters(), lr=new_learning_rate, weight_decay=0.0001)\n",
    "\n",
    "    k_fold_loss = np.empty((0,3))\n",
    "    val_scores = dict()\n",
    "    fin_val_scores = dict()\n",
    "    all_val_loss = []\n",
    "    fin_val_loss = []\n",
    "    best_models = []\n",
    "    best_all_val_loss = []\n",
    "    for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
    "        model.train()\n",
    "        print(\"Training mode\")\n",
    "        \n",
    "        # Adaptative learning rate\n",
    "        new_learning_rate = learning_rate*np.exp(-(epoch/4))\n",
    "        print(\"Learning rate:\", new_learning_rate)\n",
    "        optim = optimizer(model.parameters(), lr=new_learning_rate, weight_decay=0.0001)\n",
    "\n",
    "        \n",
    "        # validation losses for this epoch (n=k_fold)\n",
    "        val_loss = []\n",
    "        k = 0\n",
    "        for train_indices, val_indices in KFold(n_splits=k_fold).split(list(range(len(train_ds)))):\n",
    "            # k-fold cross validation\n",
    "            # k-fold dataloader (k=3) - Take validation subset for training, to avoid overfit\n",
    "    \n",
    "            train_subset = Subset(train_ds, train_indices)\n",
    "            val_subset = Subset(train_ds, val_indices)\n",
    "\n",
    "            ##### unbalanced sampler for unbalanced datasets! #####\n",
    "            y_data = train_subset.dataset.y_data[train_indices]\n",
    "            \n",
    "            class_sample_count = [len(y_data[y_data==sleep_class]) for sleep_class in range(5)]\n",
    "            class_weights = 1/torch.Tensor(class_sample_count)\n",
    "\n",
    "            # some handmade adjustments \n",
    "            class_weights[1] /= 1.6\n",
    "            class_weights[0] = class_weights[-1]\n",
    "            class_weights[2] = class_weights[-1]\n",
    "            class_weights[3] = class_weights[-1]*1.4\n",
    "            class_weights[-1] *= 1.4\n",
    "\n",
    "            weights = [class_weights[y_data[i]] for i in range(len(y_data))]\n",
    "            \n",
    "            # feed weights to loss criterion and sampler \n",
    "            criterion = loss_func(weight=class_weights.to(device))\n",
    "            sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(y_data))\n",
    "            #######################################################\n",
    "            \n",
    "            # train_dataloader with WeightedRandomSampler!\n",
    "            train_dataloader = DataLoader(train_subset, batch_size=bs, num_workers=4, sampler=sampler)\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(train_dataloader, 0):\n",
    "                \n",
    "                # get the inputs; data is a tuple of (inputs, labels)\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optim.zero_grad()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward() # retropropagating error\n",
    "                optim.step() \n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if i % 100 == 99:\n",
    "                    print('[%d, %5d] loss: %.4f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 100))\n",
    "\n",
    "                    running_loss = 0.0\n",
    "\n",
    "                \n",
    "            # k-fold validation dataset: the k-fold data set that has not been used for the last training epoch \n",
    "            val_dataloader = DataLoader(val_subset, batch_size=2*bs, num_workers=8)\n",
    "            val_loss += [loss_val(model, val_dataloader, criterion)]\n",
    "            \n",
    "            # scores for the k-fold \n",
    "            val_scores['epoch_'+str(epoch)+'k_'+str(k)] = evaluate_scores(model, val_dataloader)\n",
    "\n",
    "            k += 1\n",
    "\n",
    "        \n",
    "        # keeping track of loss for each k-fold validation for each epoch\n",
    "        k_fold_loss = np.append(k_fold_loss, np.array([val_loss]), axis=0)\n",
    "        \n",
    "        # keeping track of mean loss for each epoch\n",
    "        all_val_loss += [np.round(np.mean(val_loss), 5)]\n",
    "        print('K-fold validation loss:', all_val_loss)\n",
    "        \n",
    "        ######################################################\n",
    "        # Updating models so it returns the best model related to all_val_loss\n",
    "        if epoch == 0: # first epoch, save model\n",
    "            best_model = model.state_dict()\n",
    "            best_all_val_loss = all_val_loss[-1]\n",
    "        else: # update the model by the new one if new one is better\n",
    "            msg = \"Model not updated!\"\n",
    "            if all_val_loss[-1] < best_all_val_loss:\n",
    "                best_model = model.state_dict()\n",
    "                best_all_val_loss = all_val_loss[-1]\n",
    "                msg = \"Best model updated!\"\n",
    "            print(msg, best_all_val_loss)\n",
    "        ######################################################\n",
    "            \n",
    "        \n",
    "        # final validation dataset: has not been used for the training\n",
    "        fin_val_dataloader = DataLoader(val_ds, batch_size=bs, num_workers=8)\n",
    "        fin_val_loss += [np.round(loss_val(model, fin_val_dataloader, loss_func()),5)]\n",
    "        print('Final validation loss:', fin_val_loss)\n",
    "        fin_val_scores['epoch_'+str(epoch)] = evaluate_scores(model, fin_val_dataloader)\n",
    "        print(fin_val_scores['epoch_'+str(epoch)])\n",
    "\n",
    "    print('Finished training!')\n",
    "    return best_model, all_val_loss, k_fold_loss, val_scores, fin_val_loss, fin_val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T15:50:31.671424Z",
     "iopub.status.busy": "2021-01-04T15:50:31.670409Z",
     "iopub.status.idle": "2021-01-04T15:50:31.677714Z",
     "shell.execute_reply": "2021-01-04T15:50:31.678246Z"
    },
    "papermill": {
     "duration": 0.383673,
     "end_time": "2021-01-04T15:50:31.678392",
     "exception": false,
     "start_time": "2021-01-04T15:50:31.294719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:  True\n"
     ]
    }
   ],
   "source": [
    "print('GPU: ', torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014446,
     "end_time": "2021-01-04T15:50:31.708763",
     "exception": false,
     "start_time": "2021-01-04T15:50:31.694317",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Initializing datasets\n",
    "- Choosing channels\n",
    "- k-neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T15:51:18.313305Z",
     "iopub.status.busy": "2021-01-04T15:50:45.477452Z",
     "iopub.status.idle": "2021-01-04T15:52:06.648985Z",
     "shell.execute_reply": "2021-01-04T15:52:06.649734Z"
    },
    "papermill": {
     "duration": 94.926559,
     "end_time": "2021-01-04T15:52:06.649961",
     "exception": false,
     "start_time": "2021-01-04T15:50:31.723402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel being used: ['Sxx_1', 'Sxx_2', 'Sxx_3', 'Sxx_4', 'Sxx_5', 'Sxx_6', 'Sxx_7']\n",
      "Input data shape: (7, 24688, 40, 60)\n",
      "Training subjects: [22  8  6 16 18 28 10 26 24 14  9 13  3 12 20 23 21  4  7 30  0 27 29 11]\n",
      "Validation subjects: [25 15 19  1  2  5 17]\n",
      "Channel being used: ['Sxx_1', 'Sxx_2', 'Sxx_3', 'Sxx_4', 'Sxx_5', 'Sxx_6', 'Sxx_7']\n",
      "Input data shape: (7, 24980, 40, 60)\n",
      "Test subjects: [31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54\n",
      " 55 56 57 58 59 60]\n",
      "Labels not given, dataset will return only x values\n",
      "Datasets ready!\n"
     ]
    }
   ],
   "source": [
    "occipital = ['Sxx_4', 'Sxx_5', 'Sxx_6', 'Sxx_7']\n",
    "frontal = ['Sxx_1', 'Sxx_2', 'Sxx_3']\n",
    "channels = frontal+occipital \n",
    "k_neighbor = 2\n",
    "train_ds, val_ds = get_train_validation_dataset(channel_list=channels, ratio=0.2, k_neighbor=k_neighbor)\n",
    "test_ds = get_test_dataset(channel_list=channels, k_neighbor=k_neighbor)\n",
    "print('Datasets ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015866,
     "end_time": "2021-01-04T15:52:06.686859",
     "exception": false,
     "start_time": "2021-01-04T15:52:06.670993",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Initializing model and calling training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T15:52:06.725732Z",
     "iopub.status.busy": "2021-01-04T15:52:06.725024Z",
     "iopub.status.idle": "2021-01-04T16:28:43.533273Z",
     "shell.execute_reply": "2021-01-04T16:28:43.532738Z"
    },
    "papermill": {
     "duration": 2196.830696,
     "end_time": "2021-01-04T16:28:43.533396",
     "exception": false,
     "start_time": "2021-01-04T15:52:06.702700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n",
      "Training mode\n",
      "Learning rate: 0.001\n",
      "[1,   100] loss: 1.9611\n",
      "[1,   200] loss: 1.0359\n",
      "[1,   100] loss: 0.9000\n",
      "[1,   200] loss: 0.7747\n",
      "[1,   100] loss: 0.7088\n",
      "[1,   200] loss: 0.6394\n",
      "K-fold validation loss: [0.85801]\n",
      "Final validation loss: [0.93391]\n",
      "{'balanced_accuracy': 0.6495869793920955, 'cohen_kappa': 0.6132011342448727, 'confusion_matrix': array([[ 511,   43,   25,   10,  108],\n",
      "       [  51,   83,   78,   11,  109],\n",
      "       [ 116,   13, 1428,  284,  232],\n",
      "       [  69,    0,  154,  902,   43],\n",
      "       [  99,    5,  120,   11,  962]]), 'mean_f1_score': 0.7053155416175727}\n",
      "Training mode\n",
      "Learning rate: 0.0007788007830714049\n",
      "[2,   100] loss: 0.7122\n",
      "[2,   200] loss: 0.6228\n",
      "[2,   100] loss: 0.6608\n",
      "[2,   200] loss: 0.6058\n",
      "[2,   100] loss: 0.5592\n",
      "[2,   200] loss: 0.5106\n",
      "K-fold validation loss: [0.85801, 0.70091]\n",
      "Best model updated! 0.70091\n",
      "Final validation loss: [0.93391, 0.83572]\n",
      "{'balanced_accuracy': 0.6988076844520633, 'cohen_kappa': 0.6581736942786134, 'confusion_matrix': array([[ 602,   17,   26,    9,   43],\n",
      "       [  57,  108,   59,    3,  105],\n",
      "       [  71,   81, 1475,  120,  326],\n",
      "       [  11,    2,  264,  863,   28],\n",
      "       [  51,   47,   74,    2, 1023]]), 'mean_f1_score': 0.7420254372493736}\n",
      "Training mode\n",
      "Learning rate: 0.0006065306597126335\n",
      "[3,   100] loss: 0.5170\n",
      "[3,   200] loss: 0.4796\n",
      "[3,   100] loss: 0.5145\n",
      "[3,   200] loss: 0.5126\n",
      "[3,   100] loss: 0.4496\n",
      "[3,   200] loss: 0.4082\n",
      "K-fold validation loss: [0.85801, 0.70091, 0.54135]\n",
      "Best model updated! 0.54135\n",
      "Final validation loss: [0.93391, 0.83572, 1.35904]\n",
      "{'balanced_accuracy': 0.6923441788380146, 'cohen_kappa': 0.6195194357627585, 'confusion_matrix': array([[ 587,   29,    8,    3,   70],\n",
      "       [  68,  133,   34,    8,   89],\n",
      "       [ 108,  142, 1252,  210,  361],\n",
      "       [  25,   12,  150,  972,    9],\n",
      "       [  86,   48,   89,   37,  937]]), 'mean_f1_score': 0.7090645755643865}\n",
      "Training mode\n",
      "Learning rate: 0.0004723665527410147\n",
      "[4,   100] loss: 0.4166\n",
      "[4,   200] loss: 0.3832\n",
      "[4,   100] loss: 0.4331\n",
      "[4,   200] loss: 0.3881\n",
      "[4,   100] loss: 0.4013\n",
      "[4,   200] loss: 0.3365\n",
      "K-fold validation loss: [0.85801, 0.70091, 0.54135, 0.4359]\n",
      "Best model updated! 0.4359\n",
      "Final validation loss: [0.93391, 0.83572, 1.35904, 1.30645]\n",
      "{'balanced_accuracy': 0.6804270145706214, 'cohen_kappa': 0.6116974302382892, 'confusion_matrix': array([[ 582,   41,   21,    2,   51],\n",
      "       [  67,  121,   55,    5,   84],\n",
      "       [ 126,  109, 1248,  273,  317],\n",
      "       [  25,    3,   83, 1046,   11],\n",
      "       [ 138,  121,   78,   16,  844]]), 'mean_f1_score': 0.7026495814130592}\n",
      "Training mode\n",
      "Learning rate: 0.00036787944117144236\n",
      "[5,   100] loss: 0.3507\n",
      "[5,   200] loss: 0.3064\n",
      "[5,   100] loss: 0.3468\n",
      "[5,   200] loss: 0.3338\n",
      "[5,   100] loss: 0.3076\n",
      "[5,   200] loss: 0.2736\n",
      "K-fold validation loss: [0.85801, 0.70091, 0.54135, 0.4359, 0.35793]\n",
      "Best model updated! 0.35793\n",
      "Final validation loss: [0.93391, 0.83572, 1.35904, 1.30645, 1.46361]\n",
      "{'balanced_accuracy': 0.6903256951454994, 'cohen_kappa': 0.6180294390495595, 'confusion_matrix': array([[ 611,   38,   13,    7,   28],\n",
      "       [  82,  131,   49,   16,   54],\n",
      "       [ 182,   98, 1273,  299,  221],\n",
      "       [  28,    6,   87, 1044,    3],\n",
      "       [ 221,   76,   68,   27,  805]]), 'mean_f1_score': 0.7067913502880688}\n",
      "Training mode\n",
      "Learning rate: 0.0002865047968601901\n",
      "[6,   100] loss: 0.2713\n",
      "[6,   200] loss: 0.2590\n",
      "[6,   100] loss: 0.3064\n",
      "[6,   200] loss: 0.2589\n",
      "[6,   100] loss: 0.2627\n",
      "[6,   200] loss: 0.2179\n",
      "K-fold validation loss: [0.85801, 0.70091, 0.54135, 0.4359, 0.35793, 0.29537]\n",
      "Best model updated! 0.29537\n",
      "Final validation loss: [0.93391, 0.83572, 1.35904, 1.30645, 1.46361, 1.54657]\n",
      "{'balanced_accuracy': 0.6910367990717431, 'cohen_kappa': 0.6234953091927993, 'confusion_matrix': array([[ 616,   30,   15,    6,   30],\n",
      "       [  74,  107,   50,   19,   82],\n",
      "       [ 116,   95, 1182,  333,  347],\n",
      "       [  30,    4,   53, 1074,    7],\n",
      "       [ 151,   51,   70,   16,  909]]), 'mean_f1_score': 0.7048067243883352}\n",
      "Training mode\n",
      "Learning rate: 0.00022313016014842982\n",
      "[7,   100] loss: 0.2131\n",
      "[7,   200] loss: 0.2028\n",
      "[7,   100] loss: 0.2495\n",
      "[7,   200] loss: 0.2131\n",
      "[7,   100] loss: 0.1841\n",
      "[7,   200] loss: 0.1793\n",
      "K-fold validation loss: [0.85801, 0.70091, 0.54135, 0.4359, 0.35793, 0.29537, 0.18866]\n",
      "Best model updated! 0.18866\n",
      "Final validation loss: [0.93391, 0.83572, 1.35904, 1.30645, 1.46361, 1.54657, 1.44476]\n",
      "{'balanced_accuracy': 0.7043256107419361, 'cohen_kappa': 0.634163994557347, 'confusion_matrix': array([[ 608,   43,   16,    8,   22],\n",
      "       [  54,  157,   61,   18,   42],\n",
      "       [  73,  151, 1443,  215,  191],\n",
      "       [  11,    6,  183,  965,    3],\n",
      "       [  81,  146,  178,    9,  783]]), 'mean_f1_score': 0.7275654923145971}\n",
      "Training mode\n",
      "Learning rate: 0.00017377394345044513\n",
      "[8,   100] loss: 0.1883\n",
      "[8,   200] loss: 0.1598\n",
      "[8,   100] loss: 0.1729\n",
      "[8,   200] loss: 0.1551\n",
      "[8,   100] loss: 0.1390\n",
      "[8,   200] loss: 0.1319\n",
      "K-fold validation loss: [0.85801, 0.70091, 0.54135, 0.4359, 0.35793, 0.29537, 0.18866, 0.13849]\n",
      "Best model updated! 0.13849\n",
      "Final validation loss: [0.93391, 0.83572, 1.35904, 1.30645, 1.46361, 1.54657, 1.44476, 1.93835]\n",
      "{'balanced_accuracy': 0.7044708737196205, 'cohen_kappa': 0.651636084389376, 'confusion_matrix': array([[ 622,   34,   18,    7,   16],\n",
      "       [  74,  126,   65,   20,   47],\n",
      "       [ 119,   92, 1446,  283,  133],\n",
      "       [  34,    2,   92, 1040,    0],\n",
      "       [ 110,  107,  179,    8,  793]]), 'mean_f1_score': 0.7357733612651329}\n",
      "Training mode\n",
      "Learning rate: 0.0001353352832366127\n",
      "[9,   100] loss: 0.1386\n",
      "[9,   200] loss: 0.1217\n",
      "[9,   100] loss: 0.1448\n",
      "[9,   200] loss: 0.1313\n",
      "[9,   100] loss: 0.1287\n",
      "[9,   200] loss: 0.1082\n",
      "K-fold validation loss: [0.85801, 0.70091, 0.54135, 0.4359, 0.35793, 0.29537, 0.18866, 0.13849, 0.09856]\n",
      "Best model updated! 0.09856\n",
      "Final validation loss: [0.93391, 0.83572, 1.35904, 1.30645, 1.46361, 1.54657, 1.44476, 1.93835, 1.85301]\n",
      "{'balanced_accuracy': 0.6988286596157212, 'cohen_kappa': 0.6338060543116179, 'confusion_matrix': array([[ 621,   35,   10,    4,   27],\n",
      "       [  73,  144,   55,   16,   44],\n",
      "       [ 113,  123, 1443,  227,  167],\n",
      "       [  26,    5,  139,  991,    7],\n",
      "       [ 125,  165,  151,    8,  748]]), 'mean_f1_score': 0.7251069150742651}\n",
      "Training mode\n",
      "Learning rate: 0.00010539922456186434\n",
      "[10,   100] loss: 0.1045\n",
      "[10,   200] loss: 0.1043\n",
      "[10,   100] loss: 0.1182\n",
      "[10,   200] loss: 0.0974\n",
      "[10,   100] loss: 0.1015\n",
      "[10,   200] loss: 0.0825\n",
      "K-fold validation loss: [0.85801, 0.70091, 0.54135, 0.4359, 0.35793, 0.29537, 0.18866, 0.13849, 0.09856, 0.06501]\n",
      "Best model updated! 0.06501\n",
      "Final validation loss: [0.93391, 0.83572, 1.35904, 1.30645, 1.46361, 1.54657, 1.44476, 1.93835, 1.85301, 2.11506]\n",
      "{'balanced_accuracy': 0.7013595918851955, 'cohen_kappa': 0.6534303775136794, 'confusion_matrix': array([[ 589,   40,   22,    7,   39],\n",
      "       [  57,  135,   74,   17,   49],\n",
      "       [ 105,   75, 1535,  212,  146],\n",
      "       [  20,    4,  173,  968,    3],\n",
      "       [ 111,   87,  172,    6,  821]]), 'mean_f1_score': 0.7403456076841982}\n",
      "Training mode\n",
      "Learning rate: 8.20849986238988e-05\n",
      "[11,   100] loss: 0.0823\n",
      "[11,   200] loss: 0.0788\n",
      "[11,   100] loss: 0.0806\n",
      "[11,   200] loss: 0.0854\n",
      "[11,   100] loss: 0.0726\n",
      "[11,   200] loss: 0.0663\n",
      "K-fold validation loss: [0.85801, 0.70091, 0.54135, 0.4359, 0.35793, 0.29537, 0.18866, 0.13849, 0.09856, 0.06501, 0.05161]\n",
      "Best model updated! 0.05161\n",
      "Final validation loss: [0.93391, 0.83572, 1.35904, 1.30645, 1.46361, 1.54657, 1.44476, 1.93835, 1.85301, 2.11506, 2.10966]\n",
      "{'balanced_accuracy': 0.7002212380236784, 'cohen_kappa': 0.6480484925236413, 'confusion_matrix': array([[ 613,   25,   15,    6,   38],\n",
      "       [  71,  123,   66,   21,   51],\n",
      "       [ 116,   73, 1449,  230,  205],\n",
      "       [  16,    4,  132, 1004,   12],\n",
      "       [ 119,   69,  159,   21,  829]]), 'mean_f1_score': 0.7326076735415162}\n",
      "Training mode\n",
      "Learning rate: 6.392786120670758e-05\n",
      "[12,   100] loss: 0.0693\n",
      "[12,   200] loss: 0.0625\n",
      "[12,   100] loss: 0.0762\n",
      "[12,   200] loss: 0.0665\n",
      "[12,   100] loss: 0.0548\n",
      "[12,   200] loss: 0.0498\n",
      "K-fold validation loss: [0.85801, 0.70091, 0.54135, 0.4359, 0.35793, 0.29537, 0.18866, 0.13849, 0.09856, 0.06501, 0.05161, 0.03666]\n",
      "Best model updated! 0.03666\n",
      "Final validation loss: [0.93391, 0.83572, 1.35904, 1.30645, 1.46361, 1.54657, 1.44476, 1.93835, 1.85301, 2.11506, 2.10966, 2.34876]\n",
      "{'balanced_accuracy': 0.7012509306498853, 'cohen_kappa': 0.6430135283485278, 'confusion_matrix': array([[ 630,   24,   13,    5,   25],\n",
      "       [  74,  123,   62,   20,   53],\n",
      "       [ 134,   96, 1396,  274,  173],\n",
      "       [  25,    6,  106, 1027,    4],\n",
      "       [ 139,   72,  166,    7,  813]]), 'mean_f1_score': 0.7275009548871276}\n",
      "Training mode\n",
      "Learning rate: 4.9787068367863945e-05\n",
      "[13,   100] loss: 0.0530\n",
      "[13,   200] loss: 0.0488\n",
      "[13,   100] loss: 0.0627\n",
      "[13,   200] loss: 0.0498\n",
      "[13,   100] loss: 0.0423\n",
      "[13,   200] loss: 0.0427\n",
      "K-fold validation loss: [0.85801, 0.70091, 0.54135, 0.4359, 0.35793, 0.29537, 0.18866, 0.13849, 0.09856, 0.06501, 0.05161, 0.03666, 0.03159]\n",
      "Best model updated! 0.03159\n",
      "Final validation loss: [0.93391, 0.83572, 1.35904, 1.30645, 1.46361, 1.54657, 1.44476, 1.93835, 1.85301, 2.11506, 2.10966, 2.34876, 2.43426]\n",
      "{'balanced_accuracy': 0.6978584321384421, 'cohen_kappa': 0.6463812909815678, 'confusion_matrix': array([[ 617,   22,   21,    7,   30],\n",
      "       [  68,  120,   72,   19,   53],\n",
      "       [ 121,   73, 1460,  254,  165],\n",
      "       [  18,    3,  160,  982,    5],\n",
      "       [ 126,   66,  157,   13,  835]]), 'mean_f1_score': 0.7317649626743358}\n",
      "Training mode\n",
      "Learning rate: 3.877420783172201e-05\n",
      "[14,   100] loss: 0.0454\n",
      "[14,   200] loss: 0.0441\n",
      "[14,   100] loss: 0.0569\n",
      "[14,   200] loss: 0.0424\n",
      "[14,   100] loss: 0.0423\n",
      "[14,   200] loss: 0.0376\n",
      "K-fold validation loss: [0.85801, 0.70091, 0.54135, 0.4359, 0.35793, 0.29537, 0.18866, 0.13849, 0.09856, 0.06501, 0.05161, 0.03666, 0.03159, 0.02224]\n",
      "Best model updated! 0.02224\n",
      "Final validation loss: [0.93391, 0.83572, 1.35904, 1.30645, 1.46361, 1.54657, 1.44476, 1.93835, 1.85301, 2.11506, 2.10966, 2.34876, 2.43426, 2.483]\n",
      "{'balanced_accuracy': 0.6954180958540284, 'cohen_kappa': 0.6523120092095742, 'confusion_matrix': array([[ 607,   32,   25,    8,   25],\n",
      "       [  56,  115,   90,   18,   53],\n",
      "       [ 106,   60, 1521,  231,  155],\n",
      "       [  19,    4,  140,  997,    8],\n",
      "       [ 112,   80,  191,    9,  805]]), 'mean_f1_score': 0.737250696985183}\n",
      "Training mode\n",
      "Learning rate: 3.01973834223185e-05\n",
      "[15,   100] loss: 0.0399\n",
      "[15,   200] loss: 0.0388\n",
      "[15,   100] loss: 0.0422\n",
      "[15,   200] loss: 0.0387\n",
      "[15,   100] loss: 0.0353\n",
      "[15,   200] loss: 0.0319\n",
      "K-fold validation loss: [0.85801, 0.70091, 0.54135, 0.4359, 0.35793, 0.29537, 0.18866, 0.13849, 0.09856, 0.06501, 0.05161, 0.03666, 0.03159, 0.02224, 0.01751]\n",
      "Best model updated! 0.01751\n",
      "Final validation loss: [0.93391, 0.83572, 1.35904, 1.30645, 1.46361, 1.54657, 1.44476, 1.93835, 1.85301, 2.11506, 2.10966, 2.34876, 2.43426, 2.483, 2.63645]\n",
      "{'balanced_accuracy': 0.7000991943129744, 'cohen_kappa': 0.6538831413042685, 'confusion_matrix': array([[ 625,   26,   21,    7,   18],\n",
      "       [  69,  117,   77,   19,   50],\n",
      "       [ 117,   75, 1498,  238,  145],\n",
      "       [  18,    4,  136, 1004,    6],\n",
      "       [ 135,   82,  169,   10,  801]]), 'mean_f1_score': 0.7377571707088187}\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Dreem_CNN(channels=len(channels),\n",
    "                  k_neighbors=k_neighbor,\n",
    "                  use_attention=True)\n",
    "\n",
    "best_model, all_val_loss, k_fold_loss, val_scores, fin_val_loss, fin_val_scores = train_model(model=model, \n",
    "                                                                                              n_epoch=15,\n",
    "                                                                                              batch_size=64,\n",
    "                                                                                              k_fold=3,\n",
    "                                                                                              learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T16:28:43.684665Z",
     "iopub.status.busy": "2021-01-04T16:28:43.683660Z",
     "iopub.status.idle": "2021-01-04T16:28:43.843975Z",
     "shell.execute_reply": "2021-01-04T16:28:43.843433Z"
    },
    "papermill": {
     "duration": 0.248961,
     "end_time": "2021-01-04T16:28:43.844098",
     "exception": false,
     "start_time": "2021-01-04T16:28:43.595137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def build_result_dict():\n",
    "    results_dict = {\n",
    "        'model': model,\n",
    "        'best_models': best_model,\n",
    "        'channels': channels,\n",
    "        'all_val_loss': all_val_loss,\n",
    "        'k_fold_loss': k_fold_loss,\n",
    "        'val_scores': val_scores,\n",
    "        'fin_val_loss': fin_val_loss,\n",
    "        'fin_val_scores': fin_val_scores,\n",
    "    }\n",
    "    return results_dict\n",
    "\n",
    "# saving results\n",
    "results_dict = build_result_dict()\n",
    "results_file = open(\"Dreem_CNN_results.pkl\", \"wb\")\n",
    "pickle.dump(results_dict, results_file)\n",
    "results_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T16:28:43.982305Z",
     "iopub.status.busy": "2021-01-04T16:28:43.970984Z",
     "iopub.status.idle": "2021-01-04T16:28:44.110058Z",
     "shell.execute_reply": "2021-01-04T16:28:44.109468Z"
    },
    "papermill": {
     "duration": 0.204334,
     "end_time": "2021-01-04T16:28:44.110174",
     "exception": false,
     "start_time": "2021-01-04T16:28:43.905840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "working_dir = os.getcwd()\n",
    "\n",
    "filename = 'best_model.pth'\n",
    "torch.save(best_model, os.path.join(working_dir, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.061545,
     "end_time": "2021-01-04T16:28:44.233857",
     "exception": false,
     "start_time": "2021-01-04T16:28:44.172312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Testing the best model\n",
    "There is not an actual test, because we don't have the ground truth. It just creates the .csv file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T16:28:44.370602Z",
     "iopub.status.busy": "2021-01-04T16:28:44.369691Z",
     "iopub.status.idle": "2021-01-04T16:29:02.745289Z",
     "shell.execute_reply": "2021-01-04T16:29:02.744140Z"
    },
    "papermill": {
     "duration": 18.449367,
     "end_time": "2021-01-04T16:29:02.745421",
     "exception": false,
     "start_time": "2021-01-04T16:28:44.296054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx_absolute = np.arange(len(train_ds) + len(val_ds), len(train_ds) + len(val_ds) + len(test_ds))\n",
    "test_model = Dreem_CNN(channels=len(channels),\n",
    "                  k_neighbors=k_neighbor,\n",
    "                  use_attention=True)\n",
    "\n",
    "filename = 'best_model.pth'\n",
    "test_model.load_state_dict(torch.load(filename))\n",
    "test_model.to(device)\n",
    "with torch.no_grad(): # do not forget to remove gradient computing during evaluation !!!\n",
    "\n",
    "    prediction_list = torch.empty(0).to(device)\n",
    "    test_model.eval()\n",
    "\n",
    "    test_dataloader = DataLoader(test_ds, batch_size=128, num_workers=4)\n",
    "    for inputs in test_dataloader:\n",
    "        outputs = test_model(inputs.to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        prediction_list = torch.cat([prediction_list, predicted])\n",
    "\n",
    "    prediction_list = prediction_list.cpu().numpy()\n",
    "\n",
    "y_p = np.stack((idx_absolute, prediction_list))\n",
    "y_p = y_p.astype(int)\n",
    "df = pd.DataFrame(y_p.T, columns = ['index','sleep_stage'])\n",
    "df.to_csv(\"y_predict.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.092932,
     "end_time": "2021-01-04T16:29:02.900176",
     "exception": false,
     "start_time": "2021-01-04T16:29:02.807244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 2320.534195,
   "end_time": "2021-01-04T16:29:04.626541",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-04T15:50:24.092346",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
