{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py  # Read and write HDF5 files from Python\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import stft\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Lambda, BatchNormalization, MaxPooling1D, Dense, Conv1D, Dropout, GRU, \\\n",
    "    TimeDistributed, \\\n",
    "    GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "data_path = r\"C:\\Users\\eidan\\Documents\\BME_Dreem\\new\"\n",
    "file_xtrain = data_path + r\"/X_train.h5\"\n",
    "file_xtest = data_path + r\"/X_test.h5\"\n",
    "file_ytrain = data_path + r\"/y_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(eeg_array):\n",
    "    \"\"\"normalize signal between 0 and 1\"\"\"\n",
    "    normalized_array = np.clip(eeg_array, -150, 150)\n",
    "    normalized_array = normalized_array / 150\n",
    "\n",
    "    return normalized_array\n",
    "\n",
    "\n",
    "def stft_preprocessing(data, mean=None, var=None):\n",
    "    \"\"\"Transform the signal in input in his STFT version, add a dimension\"\"\"\n",
    "\n",
    "    zxx = stft(data, fs=50, nperseg=128, nfft=128, noverlap=64, axis=1)[2]\n",
    "    zxx = np.log(np.abs(zxx))\n",
    "    cliped = np.clip(zxx, -20, 20)\n",
    "    cliped = np.swapaxes(cliped, 1, 2)\n",
    "    cliped = np.swapaxes(cliped, 2, 3)\n",
    "\n",
    "    newdata = cliped\n",
    "    print(newdata.shape)\n",
    "    if mean is None:\n",
    "        mean = newdata.mean()\n",
    "\n",
    "    newdata = newdata - mean\n",
    "\n",
    "    if var is None:\n",
    "        var = newdata.var()\n",
    "\n",
    "    newdata = newdata / var\n",
    "\n",
    "    return newdata, mean, var\n",
    "\n",
    "\n",
    "def positional_embeding(data):\n",
    "    \"\"\"computes the embedded position\"\"\"\n",
    "    data = np.array(data)\n",
    "\n",
    "    pos = np.zeros([data.shape[0], 6])\n",
    "\n",
    "    pos[:, 0] = data / 1200\n",
    "\n",
    "    angle = [30, 60, 90, 120, 150]\n",
    "    for i in range(5):\n",
    "        pos[:, i + 1] = np.cos((data * np.pi) / angle[i])\n",
    "\n",
    "    return pos\n",
    "\n",
    "\n",
    "def split_data(input_list):\n",
    "    \"\"\"prepare the train data\n",
    "    output:\n",
    "    x_eeg_raw_data - raw signal data\n",
    "    x_stft_data - STFT from the raw signals\n",
    "    x_emb_pos_data - embedded position\n",
    "    \"\"\"\n",
    "    # read raw data\n",
    "    with h5py.File(file_xtrain, \"r\") as fi:\n",
    "        if len(input_list) == 1:\n",
    "            x_data = fi[input_list[0]][()]\n",
    "        else:\n",
    "            x_data = np.zeros([24688, 1500, len(input_list)], dtype=np.float64)\n",
    "            for i in range(0, len(input_list)):\n",
    "                if 'x' in input_list[i] or 'y' in input_list[i] or 'z' in input_list[i]:\n",
    "                    f1 = interp1d(np.arange(0, 300), fi[input_list[i]][()], axis=1)\n",
    "                    xnew = np.linspace(0, 30, num=1500)\n",
    "                    x_data[0:24688, 0:1500, i] = f1(xnew)\n",
    "                else:\n",
    "                    x_data[0:24688, 0:1500, i] = fi[input_list[i]][()]\n",
    "\n",
    "        y_data_org = pd.read_csv(file_ytrain)['sleep_stage'].to_numpy()\n",
    "        metadata = fi[\"index_window\"][()]\n",
    "\n",
    "        mask_eeg = [0, 1, 2, 3]\n",
    "        # normalize data \n",
    "        x_eeg_raw_data = normalize_data(x_data[:, :, mask_eeg])\n",
    "        x_stft_data = np.copy(x_eeg_raw_data)\n",
    "        # kill channels to avoid overfit\n",
    "        for subjects in range(0, x_stft_data.shape[0]):\n",
    "            for channels in range(0, x_stft_data.shape[2]):\n",
    "                if np.random.uniform() < 0.1:\n",
    "                    x_stft_data[subjects, :, channels] = x_stft_data[subjects, :, channels] * 0\n",
    "        x_eeg_raw_data = np.copy(x_stft_data)\n",
    "        # create STFT\n",
    "        x_stft_data, mean_eeg, var_eeg = stft_preprocessing(x_stft_data)\n",
    "        # create embedded postions\n",
    "        x_emb_pos_data = positional_embeding(metadata)\n",
    "        # reshape data for taking into account the temporal context \n",
    "        x_emb_pos_data = np.append(x_emb_pos_data[0:-1, :], x_emb_pos_data[1:, :], axis=1)\n",
    "        x_emb_pos_data = x_emb_pos_data.reshape(24687, 2, 6)\n",
    "        x_stft_data = np.append(x_stft_data[0:-1, :, :, :], x_stft_data[1:, :, :, :], axis=1)\n",
    "        x_stft_data = x_stft_data.reshape(24687, 2, 4, 25, 65)\n",
    "\n",
    "        x_eeg_raw_data = np.append(x_eeg_raw_data[0:-1, :, :], x_eeg_raw_data[1:, :, :], axis=1)\n",
    "\n",
    "        y_data_org = y_data_org[1:]\n",
    "\n",
    "        print('data ready')\n",
    "    return x_eeg_raw_data, x_stft_data, x_emb_pos_data, y_data_org\n",
    "\n",
    "input_signals_list = ['eeg_4', 'eeg_5', 'eeg_6', 'eeg_7', 'x', 'y', 'z']\n",
    "\n",
    "x_eeg_raw, x_stft, x_emb_pos, y_data = split_data(input_signals_list)\n",
    "\n",
    "\n",
    "#make sure the data have the same shapes\n",
    "print('y_data.shape = ', y_data.shape)\n",
    "print('x_eeg.shape = ', x_stft.shape)\n",
    "print('x_eeg_normalized.shape = ', x_eeg_raw.shape)\n",
    "print('x_meta.shape = ', x_emb_pos.shape)\n",
    "shapes = np.array([y_data.shape[0], x_stft.shape[0], x_eeg_raw.shape[0], x_emb_pos.shape[0]])\n",
    "if np.sum(np.abs(np.diff(shapes))) != 0:\n",
    "    print(\"shape error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "\n",
    "def custom_loss(ytrue, ypred):\n",
    "    # this is SparseCategoricalCrossentropy with weights\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    weight = tf.constant([[0.94355903, 2.70683738, 0.96141876, 0.55233263, 0.62986756]])\n",
    "    #     weight = tf.constant([[1.92961025, 2.68815953, 0.96311429, 1.17516681, 1.21891314]])\n",
    "\n",
    "    new_y = tf.expand_dims(ypred, axis=1)\n",
    "\n",
    "    new_weight = tf.matmul(weight, new_y, transpose_b=True)\n",
    "\n",
    "    score = scce(ytrue, ypred, sample_weight=new_weight)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "# Define the K-fold Cross Validator if we want to use sklearn\n",
    "kfold = KFold(n_splits=30, shuffle=True)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    \"\"\"Custom attention layer base on the one in Dreem open repository\"\"\"\"\n",
    "    def __init__(self, input_dim, context_size=25):\n",
    "        super(Attention, self).__init__()  # constructor of Base Layer class\n",
    "        init = tf.initializers.GlorotUniform()\n",
    "        self.context_matrix = tf.Variable(init(shape=(context_size, input_dim)), trainable=True)\n",
    "        self.context_bias = tf.Variable(init(shape=(1, context_size)), trainable=True)\n",
    "        self.context_vector = tf.Variable(init(shape=(context_size, 1)), trainable=True)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'context_matrix': self.context_matrix,\n",
    "            'context_bias': self.context_bias,\n",
    "            'context_vector': self.context_vector\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        x (tensor: batch_size,sequence length,input_dim):\n",
    "        returns x (tensor: batch_size,input_dim),  attention_weights (tensor: batch_size,sequence_length)\n",
    "        \"\"\"\n",
    "        batch_size, length, n_features = x.shape\n",
    "        x_att = tf.reshape(x, [-1, n_features])\n",
    "        u = tf.linalg.matmul(x_att, tf.transpose(self.context_matrix, perm=[1, 0])) + self.context_bias\n",
    "        u = tf.nn.tanh(u)\n",
    "        uv = tf.linalg.matmul(u, self.context_vector)\n",
    "        uv = tf.reshape(uv, [-1, length])\n",
    "        alpha = tf.nn.softmax(uv, axis=1)\n",
    "        alpha = tf.expand_dims(alpha, axis=-1)\n",
    "        x_out = alpha * x\n",
    "        x_out = tf.math.reduce_sum(x_out, axis=1)\n",
    "        #         if np.random.uniform() < 0.2:\n",
    "        #             x_out = x_out * 0\n",
    "\n",
    "        return x_out\n",
    "\n",
    "\n",
    "def custom_Kfold(n):\n",
    "    # makes sure we are training and validated on different subjects\n",
    "    with h5py.File(file_xtrain, \"r\") as fi:\n",
    "        x_metadata = fi['index'][()]\n",
    "\n",
    "    x_metadata = x_metadata[1:]\n",
    "    indice = []\n",
    "\n",
    "    for i in range(n):\n",
    "        train_indices, val_indices = np.argwhere(x_metadata % n != 0 + i), np.argwhere(x_metadata % n == 0 + i)\n",
    "        np.random.shuffle(train_indices)\n",
    "        np.random.shuffle(val_indices)\n",
    "        indice.append({\"train\": np.squeeze(train_indices), \"validation\": np.squeeze(val_indices)})\n",
    "\n",
    "    return indice\n",
    "\n",
    "def backend_reshape(x):\n",
    "    # reshape with the batch size\n",
    "    return tf.keras.backend.reshape(x, (-1, n_channels, spectrogram_length, filter_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = None\n",
    "fold_no = 1\n",
    "\n",
    "# filter and channel redudcution\n",
    "C = 4\n",
    "filter_reduc = 40\n",
    "# attention layer contaxt sizes\n",
    "att_context = 20\n",
    "\n",
    "# number of units GRU\n",
    "m1 = 128\n",
    "m2 = 128\n",
    "# for dropouts\n",
    "p1 = 0.2\n",
    "p2 = 0.2\n",
    "# shape of the STFT \n",
    "_, temporal_context, n_channels, spectrogram_length, filter_size = x_stft.shape\n",
    "number_channels = n_channels\n",
    "inputA = Input(shape=(temporal_context, n_channels, spectrogram_length, filter_size))\n",
    "inputC = Input(shape=(2, 6))\n",
    "inputD = Input(shape=134)\n",
    "inputH = Input(shape=(3000, number_channels))\n",
    "\n",
    "k = Lambda(backend_reshape)(inputA)\n",
    "#  filter redudcution\n",
    "k = TimeDistributed(Dense(filter_reduc))(k)\n",
    "k = tf.transpose(k, perm=[0, 3, 2, 1])\n",
    "#  Channel redudcution\n",
    "k = TimeDistributed(Dense(C))(k)\n",
    "k = tf.transpose(k, perm=[0, 3, 2, 1])\n",
    "k = tf.keras.layers.Dropout(p1)(k)\n",
    "\n",
    "k = tf.keras.layers.Reshape((-1, spectrogram_length))(k)\n",
    "k = tf.transpose(k, perm=[0, 2, 1])\n",
    "# bidirectional GRU\n",
    "k = tf.keras.layers.Bidirectional(GRU(m1, return_sequences=True, go_backwards=False))(k)\n",
    "k = tf.keras.layers.Dropout(p1)(k)\n",
    "k = Attention(m1 * 2, att_context)(k)\n",
    "k = tf.reshape(k, [-1, temporal_context, 2 * m1])\n",
    "model_A = Model(inputs=inputA, outputs=k)\n",
    "\n",
    "# embedded position\n",
    "m = Dense(6, activation=tf.nn.relu)(inputC)\n",
    "m = Dense(6, activation=tf.nn.relu)(m)\n",
    "m = Dense(6, activation=tf.nn.relu)(m)\n",
    "m = Dense(6, activation=tf.nn.relu)(m)\n",
    "m = Dense(6, activation=tf.nn.relu)(m)\n",
    "model_C = Model(inputs=inputC, outputs=m)\n",
    "\n",
    "# SkipGru\n",
    "combined_input = concatenate([model_A.output, model_C.output])\n",
    "combined = tf.keras.layers.Reshape((combined_input.shape[1], -1))(combined_input)\n",
    "x_residual = Dense(m1 * 2)(combined)\n",
    "x_lstm = tf.keras.layers.Bidirectional(GRU(m2, return_state=True, return_sequences=True))(combined,\n",
    "                                                                                          initial_state=hidden)  # 1\n",
    "hidden = x_lstm[1:]\n",
    "x_lstm = x_lstm[0]\n",
    "x_cat = (x_residual + x_lstm) / 2\n",
    "combined = Dropout(p2)(x_cat)\n",
    "x_residual = Dense(m1 * 2)(combined)\n",
    "x_lstm = tf.keras.layers.Bidirectional(GRU(m2, return_state=True, return_sequences=True))(combined,\n",
    "                                                                                          initial_state=hidden)  # 2\n",
    "hidden = x_lstm[1:]\n",
    "x_lstm = x_lstm[0]\n",
    "x_cat = (x_residual + x_lstm) / 2\n",
    "combined = Dropout(p2)(x_cat)\n",
    "combined = GlobalAveragePooling1D()(combined)\n",
    "\n",
    "combined = Model(inputs=[model_A.input, model_C.input], outputs=combined)\n",
    "\n",
    "# raw data 1D CNN\n",
    "raws = BatchNormalization()(inputH)\n",
    "raws = Conv1D(filters=60, kernel_size=20, padding='SAME', activation=tf.nn.leaky_relu)(raws)\n",
    "raws = MaxPooling1D(pool_size=6)(raws)\n",
    "raws = Conv1D(filters=80, kernel_size=15, padding='SAME', activation=tf.nn.leaky_relu)(raws)\n",
    "raws = MaxPooling1D(pool_size=6)(raws)\n",
    "raws = Dropout(0.25)(raws)\n",
    "raws = Conv1D(filters=80, kernel_size=10, padding='SAME', activation=tf.nn.leaky_relu)(raws)\n",
    "raws = MaxPooling1D(pool_size=6)(raws)\n",
    "raws = Conv1D(filters=80, kernel_size=10, padding='SAME', activation=tf.nn.leaky_relu)(raws)\n",
    "raws = MaxPooling1D(pool_size=4)(raws)\n",
    "raws = Dense(128, activation=tf.nn.leaky_relu)(raws)\n",
    "z = GlobalAveragePooling1D()(raws)\n",
    "\n",
    "z = Model(inputs=inputH, outputs=z)\n",
    "\n",
    "last_layer = concatenate([combined.output, z.output])\n",
    "# last layer (softmax)\n",
    "v = Dense(5, activation=\"softmax\")(last_layer)\n",
    "\n",
    "model = Model(inputs=[model_A.input, model_C.input, z.input], outputs=v)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = None\n",
    "# use one subject out CV\n",
    "dic_kfold = custom_Kfold(20)\n",
    "#run model with Kfold\n",
    "for dic in dic_kfold:\n",
    "#     train, test = dic.values()\n",
    "# for train, test in kfold.split(x_stft, y_data):\n",
    "    if fold_no == 15:\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        model.compile(loss=custom_loss,\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "        history = model.fit([x_stft[train], x_emb_pos[train], x_eeg_raw[train]], y_data[train],\n",
    "                            batch_size=32,\n",
    "                            validation_data=([x_stft[test], x_emb_pos[test], x_eeg_raw[test]], y_data[test])\n",
    "                            , callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=15)],\n",
    "                            epochs=7, verbose=1)\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'DONE training for fold {fold_no} ...')\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if history is not None:\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy with coustom loss')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss with coustom loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(data, row_labels, col_labels, ax=None,\n",
    "            cbar_kw={}, cbarlabel=\"\", **kwargs):\n",
    "    \"\"\"\n",
    "    Create a heatmap from a numpy array and two lists of labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        A 2D numpy array of shape (N, M).\n",
    "    row_labels\n",
    "        A list or array of length N with the labels for the rows.\n",
    "    col_labels\n",
    "        A list or array of length M with the labels for the columns.\n",
    "    ax\n",
    "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
    "        not provided, use current axes or create a new one.  Optional.\n",
    "    cbar_kw\n",
    "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
    "    cbarlabel\n",
    "        The label for the colorbar.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to `imshow`.\n",
    "    \"\"\"\n",
    "\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    # ... and label them with the respective list entries.\n",
    "    ax.set_xticklabels(col_labels)\n",
    "    ax.set_yticklabels(row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    for edge, spine in ax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im, cbar\n",
    "\n",
    "\n",
    "def annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\",\n",
    "                     textcolors=[\"black\", \"white\"],\n",
    "                     threshold=None, **textkw):\n",
    "    \"\"\"\n",
    "    A function to annotate a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im\n",
    "        The AxesImage to be labeled.\n",
    "    data\n",
    "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
    "    valfmt\n",
    "        The format of the annotations inside the heatmap.  This should either\n",
    "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
    "        `matplotlib.ticker.Formatter`.  Optional.\n",
    "    textcolors\n",
    "        A list or array of two color specifications.  The first is used for\n",
    "        values below a threshold, the second for those above.  Optional.\n",
    "    threshold\n",
    "        Value in data units according to which the colors from textcolors are\n",
    "        applied.  If None (the default) uses the middle of the colormap as\n",
    "        separation.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to each call to `text` used to create\n",
    "        the text labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(data, (list, np.ndarray)):\n",
    "        data = im.get_array()\n",
    "\n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max())/2.\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\",\n",
    "              verticalalignment=\"center\")\n",
    "    kw.update(textkw)\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "\n",
    "    return texts\n",
    "\n",
    "def plot_heatmap(y_true,y_pred): \n",
    "    \n",
    "    mat = confusion_matrix(y_true,y_pred)\n",
    "    mat = mat/np.sum(mat ,axis =0)*100\n",
    "    mat = mat.astype(np.int)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    im, cbar = heatmap(mat, ['Wake','NREM1','NREM2','NREM3','REM'],['Wake','NREM1','NREM2','NREM3','REM'], ax=ax,\n",
    "                       cmap=\"YlGn\", cbarlabel=\"Pourcentage de classification\")\n",
    "    texts = annotate_heatmap(im, valfmt=\"{x:} %\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "y_estimate = model.predict([x_stft[test], x_emb_pos[test], x_eeg_raw[test]])\n",
    "y_estimate = np.argmax(y_estimate, axis=1)\n",
    "print('confusion matrix with coustom loss')\n",
    "plot_heatmap(y_data[test], y_estimate)\n",
    "print('____________________________________________')\n",
    "print('')\n",
    "print('')\n",
    "print('classification report with coustom loss')\n",
    "print('')\n",
    "target_names=['Wake','NREM1','NREM2','NREM3','REM']\n",
    "# print(confusion_matrix(y_data[test], y_estimate))\n",
    "print(classification_report(y_data[test], y_estimate, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(input_list):\n",
    "    '''get the test data the same as the traning data'''\n",
    "    with h5py.File(file_xtest, \"r\") as fi:\n",
    "        index = fi['index_absolute'][()]\n",
    "        metadata = fi[\"index_window\"][()]\n",
    "        if len(input_list) == 1:\n",
    "            x_data = fi[input_list[0]][()]\n",
    "        else:\n",
    "            x_data = np.zeros([24980, 1500, len(input_list)], dtype=np.float64)\n",
    "            for i in range(0, len(input_list)):\n",
    "                if 'x' in input_list[i] or 'y' in input_list[i] or 'z' in input_list[i]:\n",
    "                    f1 = interp1d(np.arange(0, 300), fi[input_list[i]][()], axis=1)\n",
    "                    xnew = np.linspace(0, 30, num=1500)\n",
    "                    x_data[0:24980, 0:1500, i] = f1(xnew)\n",
    "                else:\n",
    "                    x_data[0:24980, 0:1500, i] = fi[input_list[i]][()]\n",
    "\n",
    "    # EEG extraction\n",
    "    mask_eeg = [0, 1, 2, 3]\n",
    "\n",
    "    x_eeg_normalized = normalize_data(x_data[:, :, mask_eeg])\n",
    "    x_eeg = np.copy(x_eeg_normalized)\n",
    "    x_eeg, mean_eeg, var_eeg = stft_preprocessing(x_eeg)\n",
    "    x_meta = positional_embeding(metadata)\n",
    "    x_meta = np.append(x_meta[0:-1, :], x_meta[1:, :], axis=1)\n",
    "    x_meta = x_meta.reshape(24979, 2, 6)\n",
    "    x_eeg = np.append(x_eeg[0:-1, :, :, :], x_eeg[1:, :, :, :], axis=1)\n",
    "    x_eeg = x_eeg.reshape(24979, 2, 4, 25, 65)\n",
    "    x_eeg_normalized = np.append(x_eeg_normalized[0:-1, :, :], x_eeg_normalized[1:, :, :], axis=1)\n",
    "\n",
    "    print('data ready')\n",
    "\n",
    "    return x_eeg_normalized, x_eeg, x_meta, index\n",
    "\n",
    "\n",
    "x_eeg_raw_test, x_stft_test, x_emb_pos_test, index = test_data(input_signals_list)\n",
    "# predict the test data and save it as CSV\n",
    "y_test = model.predict([x_stft_test, x_emb_pos_test, x_eeg_raw_test])\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "y_test = np.append(y_test[0], y_test)\n",
    "if index.shape == y_test.shape:\n",
    "    df = pd.DataFrame(data={'index': index, 'sleep_stage': y_test})\n",
    "    df.to_csv('model_num_' + str(fold_no) + '_our_result_late.csv', index=False)\n",
    "    print('you can save the results')\n",
    "else:\n",
    "    print('there is an error in the shape of y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
