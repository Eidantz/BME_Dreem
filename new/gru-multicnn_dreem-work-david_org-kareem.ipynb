{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.datasets import cifar10\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# # from mne.decoding import SPoC\n",
    "# import tensorflow.keras.backend as K\n",
    "# from keras import Input ,Model\n",
    "# from keras.layers import RepeatVector, Permute ,Activation\n",
    "# from tensorflow.keras.backend import squeeze, dot, expand_dims,tanh, exp\n",
    "# from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "# from tensorflow.keras.layers import Layer\n",
    "# from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Flatten, Conv2D, MaxPooling2D, Conv1D,MaxPool2D,GRU, Dropout, MaxPool1D, ConvLSTM2D,MaxPooling1D, ReLU, LeakyReLU, RNN, SimpleRNN, GRU, LSTM, TimeDistributed\n",
    "# from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "# from tensorflow.keras.optimizers import Adam, SGD\n",
    "# from tensorflow.keras.layers import BatchNormalization\n",
    "# from sklearn.model_selection import KFold\n",
    "# from scipy.interpolate import interp1d\n",
    "# import tensorflow as tf\n",
    "# from scipy.signal import stft, spectrogram\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# import h5py # Read and write HDF5 files from Python\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from keras import Input ,Model\n",
    "# from tensorflow.keras.layers import concatenate\n",
    "import h5py  # Read and write HDF5 files from Python\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from scipy.stats import zscore\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import stft, spectrogram\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Lambda, GlobalMaxPool1D,LSTM,BatchNormalization,MaxPooling1D, Dense, Flatten, Conv1D, Dropout, GRU, TimeDistributed, LeakyReLU, MaxPool1D, GlobalAveragePooling1D\n",
    "# from mne.decoding import SPoCMaxPooling1D,\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import scipy.io as sio\n",
    "# from hyperas.distributions import choice, uniform\n",
    "data_path = r\"C:\\Users\\eidan\\Documents\\BME_Dreem\\new\"\n",
    "file_xtrain = data_path + r\"/X_train.h5\"\n",
    "file_xtest = data_path + r\"/X_test.h5\"\n",
    "file_ytrain = data_path + r\"/y_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill them allllll\n",
      "JK just 50% of them\n",
      "ok 40% ...\n",
      "(24688, 65, 4, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-99cc04459482>:87: RuntimeWarning: divide by zero encountered in log\n",
      "  Zxx = np.log(np.abs(Zxx))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24688, 4, 25, 65)\n",
      "(24687, 12)\n",
      "(24687, 8, 25, 65)\n",
      "(24687, 2, 6)\n",
      "data ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # training labels\n",
    "# pd.read_csv(file_ytrain)\n",
    "# # # what does the h5 file contains ?\n",
    "# # with h5py.File(file_xtrain, \"r\") as hf:\n",
    "# #        print(list(hf.keys()))\n",
    "\n",
    "# # # How to load data from h5? what is its shape and type?\n",
    "# # with h5py.File(file_xtrain, \"r\") as hf:\n",
    "# #        field = list(hf.keys())[0]\n",
    "# #        x_data = hf[field][()]\n",
    "# # type(x_data), x_data.shape\n",
    "\n",
    "\n",
    "# ############################################################################################################################\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import scipy.fftpack as fft\n",
    "import scipy\n",
    "import scipy.signal\n",
    "import scipy.interpolate\n",
    "import six\n",
    "\n",
    "def power_to_db(S, ref=1.0, amin=1e-10, top_db=80.0):\n",
    "    \"\"\"Convert a power spectrogram (amplitude squared) to decibel (dB) units\"\"\"\n",
    "    \n",
    "    S = np.asarray(S)\n",
    "\n",
    "    if amin <= 0:\n",
    "        raise ParameterError('amin must be strictly positive')\n",
    "\n",
    "    if np.issubdtype(S.dtype, np.complexfloating):\n",
    "        warnings.warn('power_to_db was called on complex input so phase '\n",
    "                      'information will be discarded. To suppress this warning, '\n",
    "                      'call power_to_db(magphase(D, power=2)[0]) instead.')\n",
    "        magnitude = np.abs(S)\n",
    "    else:\n",
    "        magnitude = S\n",
    "\n",
    "    if six.callable(ref):\n",
    "        # User supplied a function to calculate reference power\n",
    "        ref_value = ref(magnitude)\n",
    "    else:\n",
    "        ref_value = np.abs(ref)\n",
    "\n",
    "    log_spec = 10.0 * np.log10(np.maximum(amin, magnitude))\n",
    "    log_spec -= 10.0 * np.log10(np.maximum(amin, ref_value))\n",
    "\n",
    "    if top_db is not None:\n",
    "        if top_db < 0:\n",
    "            raise ParameterError('top_db must be non-negative')\n",
    "        log_spec = np.maximum(log_spec, log_spec.max() - top_db)\n",
    "\n",
    "    return log_spec\n",
    "\n",
    "# #####################################################################################################################\n",
    "\n",
    "def normalize_data(eeg_array):\n",
    "    \"\"\"normalize signal between 0 and 1\"\"\"\n",
    "    sort = -np.sort(-np.abs(eeg_array), axis=1)\n",
    "    sort_mean = np.mean(sort[:, 0:3, :], axis=1)\n",
    "    normalized_array1 = np.copy(eeg_array)\n",
    "    for i in range(0,eeg_array.shape[0]):\n",
    "        for j in range(0,eeg_array.shape[2]):\n",
    "            normalized_array1[i,:,j] = eeg_array[i,:,j] / sort_mean[i,j]\n",
    "    normalized_array = np.clip(eeg_array, -150, 150)\n",
    "    normalized_array = normalized_array / 150\n",
    "\n",
    "    return normalized_array1\n",
    "\n",
    "\n",
    "def stft_preprocessing(data, mean= None, var=None):\n",
    "    \"\"\"Transform the signal in input in his STFT version, add a dimension\"\"\"\n",
    "    \n",
    "    shapedata = data.shape\n",
    "    \n",
    "#     newdata = np.zeros((shapedata[0],shapedata[-1], 48, 65,))\n",
    "    \n",
    "#     newdata = np.zeros((shapedata[0],shapedata[-1], 13, 129,))\n",
    "    \n",
    "#     for i in range(shapedata[-1]):\n",
    "#     _, _, Zxx  = stft(data ,fs = 50, axis= 1)\n",
    "    Zxx = stft(data, fs=50, nperseg=128, nfft=128, noverlap=64, axis=1)[2]\n",
    "    print(Zxx.shape)\n",
    "#         _, _, Zxx=  stft(data[:,:,i] ,nperseg=128,nfft=128,fs = 50, axis= 1)\n",
    "#     Zxx = power_to_db(Zxx, ref = np.mean)\n",
    "    Zxx = np.log(np.abs(Zxx))\n",
    "    cliped = np.clip(Zxx,-20,20)\n",
    "    cliped = np.swapaxes(cliped,1,2)\n",
    "    cliped = np.swapaxes(cliped,2,3)\n",
    "#         newdata[:,i,:,:] = cliped\n",
    "    \n",
    "    \n",
    "    newdata = cliped\n",
    "    print(newdata.shape)\n",
    "    if mean ==None: \n",
    "        mean = newdata.mean() \n",
    "        \n",
    "\n",
    "    newdata = newdata-mean\n",
    "  \n",
    "    if var==None :\n",
    "        var = newdata.var()\n",
    "    \n",
    "    newdata = newdata/var\n",
    "\n",
    "        \n",
    "    \n",
    "    return (newdata, mean,var)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "def SPoC_preprocessing(x_train, y_train, x_validation, n_components= 6):\n",
    "\n",
    "    x_train = np.moveaxis(x_train,1,-1)\n",
    "    spoc_train= SPoC(n_components=n_components, reg='oas', rank='full', transform_into='csp_space')\n",
    "    spoc_train.fit(x_train,y_train)\n",
    "\n",
    "    x_train_copy = spoc_train.transform(x_train)\n",
    "    x_train_copy = np.moveaxis(x_train_copy,1,-1)\n",
    "\n",
    "    x_validation_copy = np.moveaxis(x_validation,1,-1)\n",
    "    x_validation_copy = spoc_train.transform(x_validation_copy)\n",
    "    x_validation_copy = np.moveaxis(x_validation_copy,1,-1)\n",
    "    \n",
    "    return (x_train_copy,x_validation_copy, spoc_train)\n",
    "\n",
    "\n",
    " #----------------------------------------------------------------------------------------------------\n",
    "\n",
    "def coordinate_preprocessing(data, mean = None , var = None):\n",
    "    \n",
    "    \n",
    "    if mean is None: \n",
    "        mean =  data.mean(axis=(0,1))\n",
    "    else :\n",
    "        data = data-mean\n",
    "  \n",
    "    if var is None :\n",
    "        var = data.var(axis= (0,1))\n",
    "\n",
    "    else :\n",
    "        data = data/var\n",
    "    \n",
    "    \n",
    "    data = np.diff(data, axis = 1)\n",
    "    return (data , mean , var)\n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------\n",
    "def positional_embeding(data): \n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    pos = np.zeros([data.shape[0],6])\n",
    "    \n",
    "    pos[:,0]= data/1200\n",
    "    \n",
    "    \n",
    "    angle =  [30, 60, 90, 120, 150] \n",
    "    for i in range(5):\n",
    "        pos[:,i+1]=np.cos((data * np.pi)/angle[i]) \n",
    "    \n",
    "    \n",
    "    return pos \n",
    "        \n",
    "    \n",
    "    \n",
    "def split_data(input_signals_list,validation_ratio=0.3):\n",
    "    with h5py.File(file_xtrain, \"r\") as fi:\n",
    "        if len(input_signals_list) == 1:\n",
    "            x_data = fi[input_signals_list[0]][()]\n",
    "        else:\n",
    "            x_data = np.zeros([24688,1500,len(input_signals_list)], dtype=np.float64)\n",
    "            for i in range(0, len(input_signals_list)):\n",
    "                if 'x' in input_signals_list[i] or 'y' in input_signals_list[i] or 'z' in input_signals_list[i]:\n",
    "                    f1 = interp1d(np.arange(0, 300), fi[input_signals_list[i]][()], axis=1)\n",
    "                    xnew = np.linspace(0, 30, num=1500)\n",
    "                    x_data[0:24688, 0:1500, i] = f1(xnew)\n",
    "                else:\n",
    "                    x_data[0:24688, 0:1500, i] = fi[input_signals_list[i]][()]\n",
    "        \n",
    "        y_data = pd.read_csv(file_ytrain)['sleep_stage'].to_numpy()\n",
    "        metadata =  fi[\"index_window\"][()]\n",
    "        \n",
    "        \n",
    "        # Converting to 1 minutes epochs\n",
    "#         x_data = np.append(x_data[0:-1,:,:],x_data[1:,:,:],axis=1)\n",
    "#         y_data = y_data[1:]\n",
    "        \n",
    "#         metadata=np.delete(metadata,0)\n",
    "#         n1 = np.where(y_data == 1)[0]\n",
    "#         x_data_n1 = x_data[n1,:,:]\n",
    "#         for i in [0,3,5]:\n",
    "#             x_data_n1 = x_data[n1, :, :]\n",
    "#             x_data_n1[:,:,i] = 0\n",
    "#             x_data = np.append(x_data, x_data_n1, axis=0)\n",
    "#         # EEG extraction\n",
    "#         y_data=np.append(y_data,np.ones(np.abs(x_data.shape[0]-y_data.shape[0])))\n",
    "#         metadata = np.concatenate([metadata,metadata[n1],metadata[n1],metadata[n1]])\n",
    "        mask_eeg = [0,1,2,3]\n",
    "        \n",
    "        # You can't do spoc with K fold, or at least not here\n",
    "        # x_train_spoc, x_validation_spoc, spoc = SPoC_preprocessing(x_train[:,:,mask_eeg], y_train, x_validation[:,:,mask_eeg], 4)\n",
    "\n",
    "        x_eeg_normalized = normalize_data(x_data[:,:,mask_eeg]) \n",
    "        x_eeg = np.copy(x_eeg_normalized)\n",
    "#         x_eeg = np.append(x_eeg,x_eeg,axis=0)\n",
    "        print('kill them allllll')\n",
    "        for k in range(0,x_eeg.shape[0]):\n",
    "            for l in range(0,x_eeg.shape[2]):\n",
    "                if np.random.uniform() < 0.1:\n",
    "                    x_eeg[k,:,l] = x_eeg[k,:,l] * 0\n",
    "        x_eeg_normalized = np.copy(x_eeg)\n",
    "        print('JK just 50% of them')\n",
    "        print('ok 40% ...')\n",
    "#         x_eeg_normalized = np.append(x_eeg_normalized[0:-1,:,:],x_eeg_normalized[1:,:,:],axis=1)\n",
    "#         n1 = np.where(y_data==1)[0]\n",
    "#         x_eeg_normalized = np.append(x_eeg_normalized,x_eeg_normalized,axis=0)\n",
    "        \n",
    "#         y_data = np.append(y_data,y_data,axis=0)\n",
    "#         x_eeg_normalized = np.append(x_eeg_normalized,x_eeg_normalized[n1],axis=0)\n",
    "        \n",
    "#         y_data = np.append(y_data,y_data[n1],axis=0)\n",
    "#         print('strat augmntetion of data')\n",
    "#         for k in range(0,x_eeg_normalized.shape[0]):\n",
    "#             for l in range(0,x_eeg_normalized.shape[2]):\n",
    "#                 if np.random.uniform() < 0.2:\n",
    "#                     x_eeg_normalized[k,:,l] = x_eeg_normalized[k,:,l] * 0\n",
    "#         print('finish augmntetion of data')\n",
    "        x_eeg ,mean_eeg, var_eeg = stft_preprocessing(x_eeg)\n",
    "        \n",
    "        # X Y Z\n",
    "        \n",
    "        if 'x' in input_signals_list :\n",
    "            mask_mvt= np.argwhere(np.logical_or(np.logical_or(input_signals_list == \"x\", input_signals_list == \"y\" ) , input_signals_list == \"z\" ))\n",
    "#             print(mask_mvt)\n",
    "            x_mvt, mean_mvt , var_mvt = coordinate_preprocessing(x_data[:,:,mask_mvt])\n",
    "            x_mvt = x_data[:,:,mask_mvt]\n",
    "#             print(x_mvt.shape)\n",
    "            x_mvt = np.squeeze(x_mvt)\n",
    "#             print(x_mvt.shape)\n",
    "\n",
    "        \n",
    "        # index\n",
    "        x_meta = positional_embeding(metadata)\n",
    "        x_meta = np.append(x_meta[0:-1,:],x_meta[1:,:],axis=1)\n",
    "        print(x_meta.shape)\n",
    "        x_meta=x_meta.reshape(24687,2,6)\n",
    "        x_eeg = np.append(x_eeg[0:-1,:,:,:],x_eeg[1:,:,:,:],axis=1)\n",
    "        print(x_eeg.shape)\n",
    "        # x_data.shape\n",
    "        x_eeg = x_eeg.reshape(24687,2,4,25,65)\n",
    "        \n",
    "#         print(x_eeg_normalized.shape)\n",
    "        x_eeg_normalized = np.append(x_eeg_normalized[0:-1,:,:],x_eeg_normalized[1:,:,:],axis=1)\n",
    "        # x_data.shape\n",
    "        y_data = y_data[1:]\n",
    "#         x_eeg_normalized = x_eeg_normalized.reshape(24688*2-1,3000,7)\n",
    "        \n",
    "        \n",
    "#         x_meta = np.append(x_meta[0:-1,:],x_meta[1:,:],axis=1)\n",
    "        print(x_meta.shape)\n",
    "#         x_meta=x_meta.reshape(24688*2-1,2,6)\n",
    "#         y_data = np.append(y_data[1:],y_data)\n",
    "#         x_meta=x_meta[1:,]\n",
    "        \n",
    "        #x_data_to_connect = x_data[1:,:,:]\n",
    "#         x_data = np.concatenate((x_data[0:-2,:,:],x_data[1:-1,:,:],x_data[2:,:,:]),axis=1)\n",
    "#         for a in range(0,x_data.shape[0]-1):\n",
    "#             x_data_new=np.zeros([x_data.shape[0]-1,3000,x_data.shape[2]])\n",
    "#             x_data_new[a,:,:] = np.append(x_data[a,:,:],x_data[a+1,:,:],axis=0)\n",
    "#         x_data = x_data_new\n",
    "#         x_data_new = 0\n",
    "\n",
    "#        x_metadata = x_metadata[1:]\n",
    "#        print(x_metadata.shape)\n",
    "#        y_data =np.delete(y_data,0)\n",
    "        # Creating data indices for training and validation splits:\n",
    "#        dataset_size = len(y_data)\n",
    "#        indices = list(range(dataset_size))\n",
    "#        split = int((1 - validation_ratio) * dataset_size)\n",
    "#        np.random.shuffle(indices)\n",
    "\n",
    "        \n",
    "        \n",
    "#        train_indices, val_indices = np.argwhere(x_metadata%5 != 0) ,np.argwhere(x_metadata%5 == 0)\n",
    "#        train_indices, val_indices =  np.squeeze(train_indices), np.squeeze(val_indices)\n",
    "\n",
    "\n",
    "#        x_train, x_validation = x_data[train_indices], x_data[val_indices]\n",
    "#        y_train, y_validation = y_data[train_indices], y_data[val_indices]\n",
    "\n",
    "        print('data ready')\n",
    "    return x_eeg_normalized,x_eeg, x_meta,y_data\n",
    "# input_signals_list = ['eeg_1','eeg_2','eeg_3','eeg_4','eeg_5','y']\n",
    "#input_signals_list = ['eeg_3','eeg_4','eeg_5']\n",
    "#input_signals_list = ['eeg_1','eeg_2','eeg_3','eeg_4','eeg_5','eeg_6','eeg_7']\n",
    "input_signals_list = ['eeg_4','eeg_5', 'eeg_6', 'eeg_7','x','y','z']\n",
    "# input_signals_list = ['eeg_4','eeg_1']\n",
    "# input_signals_list = [ 'eeg_3', 'eeg_4', 'eeg_5']\n",
    "# input_signals_list = np.array(['eeg_3','eeg_4','eeg_5', 'eeg_6', 'eeg_7',\"x\", \"y\",\"z\"])\n",
    "# input_signals_list = np.array(['eeg_2','eeg_3','eeg_4','eeg_5', 'eeg_6',\"x\", \"y\",\"z\"])\n",
    "\n",
    "x_eeg_normalized, x_eeg, x_meta,y_data = split_data(input_signals_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_data.shape =  (24687,)\n",
      "x_eeg.shape =  (24687, 2, 4, 25, 65)\n",
      "x_eeg_normalized.shape =  (24687, 3000, 4)\n",
      "x_meta.shape =  (24687, 2, 6)\n"
     ]
    }
   ],
   "source": [
    "# # print(x_mvt.shape)\n",
    "print('y_data.shape = ',y_data.shape)\n",
    "print('x_eeg.shape = ',x_eeg.shape)\n",
    "print('x_eeg_normalized.shape = ',x_eeg_normalized.shape)\n",
    "print('x_meta.shape = ',x_meta.shape)\n",
    "# x_eeg = np.swapaxes(x_eeg,2,3)\n",
    "# print('x_eeg.shape = ',x_eeg.shape)\n",
    "# # # n1 = np.where(y_data==1)[0]\n",
    "# # x_eeg_normalized = np.append(x_eeg_normalized,x_eeg_normalized,axis=0)\n",
    "# # x_eeg = np.append(x_eeg,x_eeg,axis=0)\n",
    "# # x_meta = np.append(x_meta,x_meta,axis=0)\n",
    "# # y_data = np.append(y_data,y_data,axis=0)\n",
    "# # x_eeg_normalized = np.append(x_eeg_normalized,x_eeg_normalized[n1],axis=0)\n",
    "# # x_eeg = np.append(x_eeg,x_eeg[n1],axis=0)\n",
    "# # x_meta = np.append(x_meta,x_meta[n1],axis=0)\n",
    "# # y_data = np.append(y_data,y_data[n1],axis=0)\n",
    "# # print(y_data.shape)\n",
    "# # print(x_eeg.shape)\n",
    "# # print(x_eeg_normalized.shape)\n",
    "# # print(x_meta.shape)\n",
    "\n",
    "# # x_eeg = np.append(x_eeg[0:-1,:,:,:],x_eeg[1:,:,:,:],axis=1)\n",
    "# # x_data.shape\n",
    "# # x_eeg = x_eeg.reshape(24687,2,7,65,25)\n",
    "# mdic = {\"x_eeg_normalized\": x_eeg_normalized,\"x_eeg\": x_eeg,\"x_meta\": x_meta,\"y_data\":y_data}\n",
    "# sio.savemat(\"to_load4_data_aug.mat\", mdic)\n",
    "# print('y_data.shape = ',y_data.shape)\n",
    "# print('x_eeg.shape = ',x_eeg.shape)\n",
    "# print('x_eeg_normalized.shape = ',x_eeg_normalized.shape)\n",
    "# print('x_meta.shape = ',x_meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x_eeg_normalized = sio.loadmat(\"to_load3.mat\",matlab_compatible=True)['x_eeg_normalized']\n",
    "# x_eeg = sio.loadmat(\"to_load4.mat\",matlab_compatible=True)['x_eeg']\n",
    "# x_meta = sio.loadmat(\"to_load4.mat\",matlab_compatible=True)['x_meta']\n",
    "# y_data = sio.loadmat(\"to_load4.mat\",matlab_compatible=True)['y_data']\n",
    "# y_data = y_data.T\n",
    "# x_eeg = np.append(x_eeg[0:-1,:,:,:],x_eeg[1:,:,:,:],axis=1)\n",
    "# # x_data.shape\n",
    "# x_eeg = x_eeg.reshape(24687,2,7,25,65)\n",
    "\n",
    "# x_meta = np.append(x_meta[0:-1,:],x_meta[1:,:],axis=1)\n",
    "# print(x_meta.shape)\n",
    "# x_meta=x_meta.reshape(24687,2,6)\n",
    "\n",
    "\n",
    "# print('y_data.shape = ',y_data.shape)\n",
    "# print('x_eeg.shape = ',x_eeg.shape)\n",
    "# # print('x_eeg_normalized.shape = ',x_eeg_normalized.shape)\n",
    "# print('x_meta.shape = ',x_meta.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occurence = np.zeros((5))\n",
    "# for i in range(5):\n",
    "#     occurence[i]= np.sum(np.where(y_data == i))\n",
    "# #     print(occurence)\n",
    "# # plt.bar([1,2,3,4,5] , occurence )\n",
    "# weight =occurence/sum(occurence)\n",
    "# #     print(1- weight)\n",
    "# weight = - np.log( occurence/sum(occurence))\n",
    "# print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "def custom_loss(ytrue, ypred):\n",
    "    \n",
    "    \n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    \n",
    "#     weight =tf.constant([[0.85347313, 0.93098802, 0.6152054,  0.79114608, 0.80918737]])\n",
    "    #weight =tf.constant([[0.00001, 1.0000, 0.00001, 0.00001, 0.001]])\n",
    "    weight = tf.constant([[0.94355903, 2.70683738, 0.96141876, 0.55233263, 0.62986756]])\n",
    "#     weight = tf.constant([[1.92961025, 2.68815953, 0.96311429, 1.17516681, 1.21891314]])\n",
    "#     weight = tf.constant([[2.00664824, 2.05667881, 1.02830803, 1.63059645, 1.69141161]])\n",
    "    \n",
    "    new_y = tf.expand_dims(ypred, axis=1)\n",
    "\n",
    "\n",
    "    new_weight = tf.matmul(weight,new_y, transpose_b = True)\n",
    "    \n",
    "    score = scce(ytrue,ypred, sample_weight= new_weight)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=100, shuffle=True)\n",
    "# kfold = KFold(n_splits=40, shuffle=False)\n",
    "# # K-fold Cross Validation model evaluation\n",
    "fold_no = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self,input_dim, context_size=25, activation='tanh'):\n",
    "\n",
    "        super(Attention,self).__init__() # constructor of Base Layer class\n",
    "        init = tf.initializers.GlorotUniform()\n",
    "        self.context_matrix = tf.Variable(init(shape=(context_size,input_dim)),trainable=True)\n",
    "        self.context_bias = tf.Variable(init(shape=(1,context_size)),trainable=True)\n",
    "        self.context_vector = tf.Variable(init(shape=(context_size,1)),trainable=True)\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'context_matrix': self.context_matrix,\n",
    "            'context_bias': self.context_bias,\n",
    "            'context_vector': self.context_vector\n",
    "        })\n",
    "        return config\n",
    "    def call(self,x):\n",
    "        \"\"\"\n",
    "        x (tensor: batch_size,sequence length,input_dim):\n",
    "        returns x (tensor: batch_size,input_dim),  attention_weights (tensor: batch_size,sequence_length)\n",
    "        \"\"\"\n",
    "        batch_size, length, n_features = x.shape\n",
    "        x_att = tf.reshape(x,[-1, n_features])\n",
    "        u = tf.linalg.matmul(x_att,tf.transpose(self.context_matrix,perm=[1,0])) + self.context_bias\n",
    "        u = tf.nn.tanh(u)\n",
    "        uv = tf.linalg.matmul(u,self.context_vector)\n",
    "        uv = tf.reshape(uv, [-1, length])\n",
    "        alpha = tf.nn.softmax(uv,axis=1)\n",
    "        alpha = tf.expand_dims(alpha, axis=-1)\n",
    "        x_out = alpha * x\n",
    "        x_out=tf.math.reduce_sum(x_out, axis=1)\n",
    "#         if np.random.uniform() < 0.2:\n",
    "#             x_out = x_out * 0\n",
    "#         if np.random.uniform() < 0.7:\n",
    "#             x_out = x_out * 0\n",
    "\n",
    "\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_Kfold(n): \n",
    "    \n",
    "    with h5py.File(file_xtrain, \"r\") as fi:\n",
    "        x_metadata = fi['index'][()]\n",
    "\n",
    "    x_metadata = x_metadata[1:]\n",
    "#     x_metadata = np.append(x_metadata[1:],x_metadata,axis=0)\n",
    "    indice = []\n",
    "    \n",
    "    for i in range(n): \n",
    "        train_indices, val_indices = np.argwhere(x_metadata%n != 0+i) ,np.argwhere(x_metadata%n == 0+i)\n",
    "        np.random.shuffle(train_indices)\n",
    "        np.random.shuffle(val_indices)\n",
    "        indice.append(  {\"train\":np.squeeze(train_indices), \"validation\": np.squeeze(val_indices)})\n",
    "    \n",
    "    return(indice)\n",
    "dic_kfold = custom_Kfold(5)\n",
    "def backend_reshape(x):\n",
    "    return tf.keras.backend.reshape(x, (-1, n_channels, spectrogram_length, filter_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (bidirectional/forward_gru/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-441d709fb200>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# GRU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGRU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm1\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgo_backwards\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# k = tf.keras.layers.Bidirectional(GRU(units ,return_sequences=True, go_backwards=False))(k)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[1;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m     \u001b[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 925\u001b[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[0;32m    926\u001b[0m                                                 input_list)\n\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1115\u001b[0m           \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m               \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask, initial_state, constants)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[0mforward_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackward_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m       y = self.forward_layer(forward_inputs,\n\u001b[0m\u001b[0;32m    644\u001b[0m                              initial_state=forward_state, **kwargs)\n\u001b[0;32m    645\u001b[0m       y_rev = self.backward_layer(backward_inputs,\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m     \u001b[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 925\u001b[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[0;32m    926\u001b[0m                                                 input_list)\n\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1115\u001b[0m           \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m               \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m     \u001b[1;31m# GRU does not support constants. Ignore it during process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m     \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(self, inputs, initial_state, constants)\u001b[0m\n\u001b[0;32m    860\u001b[0m         \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    643\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mget_initial_state_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m       init_state = get_initial_state_fn(\n\u001b[0m\u001b[0;32m    646\u001b[0m           inputs=None, batch_size=batch_size, dtype=dtype)\n\u001b[0;32m    647\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[1;34m(self, inputs, batch_size, dtype)\u001b[0m\n\u001b[0;32m   1951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1952\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_initial_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1953\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_generate_zero_filled_state_for_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1954\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state_for_cell\u001b[1;34m(cell, inputs, batch_size, dtype)\u001b[0m\n\u001b[0;32m   2966\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2967\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2968\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_generate_zero_filled_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2969\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state\u001b[1;34m(batch_size_tensor, state_size, dtype)\u001b[0m\n\u001b[0;32m   2984\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreate_zeros\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2986\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_zeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mcreate_zeros\u001b[1;34m(unnested_state_size)\u001b[0m\n\u001b[0;32m   2979\u001b[0m     \u001b[0mflat_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munnested_state_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2980\u001b[0m     \u001b[0minit_state_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_size_tensor\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mflat_dims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2981\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_state_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2983\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2746\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2747\u001b[1;33m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2748\u001b[0m     \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_zeros_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2749\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mzeros\u001b[1;34m(shape, dtype, name)\u001b[0m\n\u001b[0;32m   2792\u001b[0m           \u001b[1;31m# Create a constant if it won't be very big. Otherwise create a fill\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2793\u001b[0m           \u001b[1;31m# op to prevent serialized GraphDefs from becoming too large.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2794\u001b[1;33m           \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2795\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2796\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_constant_if_small\u001b[1;34m(value, shape, dtype, name)\u001b[0m\n\u001b[0;32m   2730\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2731\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2732\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2733\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2734\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   3028\u001b[0m     \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3029\u001b[0m     \"\"\"\n\u001b[1;32m-> 3030\u001b[1;33m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0m\u001b[0;32m   3031\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0;32m   3032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\env_newfull\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m     raise NotImplementedError(\n\u001b[0m\u001b[0;32m    846\u001b[0m         \u001b[1;34m\"Cannot convert a symbolic Tensor ({}) to a numpy array.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m         \u001b[1;34m\" This error may indicate that you're trying to pass a Tensor to\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (bidirectional/forward_gru/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported"
     ]
    }
   ],
   "source": [
    "\n",
    "# units = 128\n",
    "hidden=None\n",
    "C=4\n",
    "att_context=20\n",
    "filter_reduc=40\n",
    "m1=128\n",
    "m2=128\n",
    "p1=0.2\n",
    "p2=0.2\n",
    "time_step =48 # Or 48\n",
    "fold_no=1\n",
    "_,temporal_context,n_channels, spectrogram_length, filter_size = x_eeg.shape\n",
    "number_channels = n_channels\n",
    "inputA = Input(shape=(temporal_context,n_channels, spectrogram_length, filter_size))\n",
    "#     inputB = Input(shape=(1499,3))\n",
    "# inputB = Input(shape=(2999,3))\n",
    "inputC = Input(shape=(2,6))\n",
    "inputD = Input(shape=(134))\n",
    "inputH = Input(shape=(3000,number_channels))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Channel redudcution\n",
    "k= Lambda(backend_reshape)(inputA)\n",
    "# k= tf.transpose(k,perm=[0,1,3,2])\n",
    "k= TimeDistributed(Dense(filter_reduc))(k)\n",
    "k= tf.transpose(k,perm=[0,3,2,1])\n",
    "k= TimeDistributed(Dense(C))(k)\n",
    "k= tf.transpose(k,perm=[0,3,2,1])\n",
    "k= tf.keras.layers.Dropout(p1)(k)\n",
    "\n",
    "k= tf.keras.layers.Reshape((-1, spectrogram_length))(k)\n",
    "k= tf.transpose(k,perm=[0,2,1])\n",
    "\n",
    "# GRU\n",
    "k = tf.keras.layers.Bidirectional(GRU(m1 ,return_sequences=True, go_backwards=False))(k)\n",
    "k= tf.keras.layers.Dropout(p1)(k)\n",
    "# k = tf.keras.layers.Bidirectional(GRU(units ,return_sequences=True, go_backwards=False))(k)\n",
    "# k= tf.transpose(k,perm=[0,2,1])\n",
    "# k= tf.keras.layers.Reshape((-1,m1))(k)\n",
    "k=Attention(m1*2,att_context)(k)\n",
    "# print(k.shape)\n",
    "k = tf.reshape(k, [-1,temporal_context, 2*m1])\n",
    "# k=Dropout(0.1)(k)\n",
    "model_A = Model(inputs=inputA, outputs= k )\n",
    "#     print(model_A.summary())\n",
    "# print(k.summary())\n",
    "\n",
    "\n",
    "# (B,context,2m+6)\n",
    "# m = Dense(24, activation=tf.nn.relu)(inputC)\n",
    "m = Dense(6, activation=tf.nn.relu)(inputC)\n",
    "m = Dense(6, activation=tf.nn.relu)(m)\n",
    "m = Dense(6, activation=tf.nn.relu)(m)\n",
    "m = Dense(6, activation=tf.nn.relu)(m)\n",
    "m = Dense(6, activation=tf.nn.relu)(m)\n",
    "model_C = Model(inputs=inputC, outputs=m)\n",
    "\n",
    "\n",
    "combined_input = concatenate([model_A.output, model_C.output])\n",
    "\n",
    "#     print('k',combined_input.shape)\n",
    "combined=tf.keras.layers.Reshape((combined_input.shape[1],-1))(combined_input)\n",
    "#     print('k',combined.shape)\n",
    "# combined = tf.transpose(combined,perm=[0,2,1])\n",
    "x_residual = Dense(m1*2)(combined)\n",
    "# combined = tf.transpose(combined,perm=[0,2,1])\n",
    "x_lstm = tf.keras.layers.Bidirectional(GRU(m2,return_state=True,return_sequences=True))(combined,initial_state=hidden)#1\n",
    "hidden=x_lstm[1:]\n",
    "x_lstm=x_lstm[0]\n",
    "# x_lstm = tf.transpose(x_lstm,perm=[0,2,1])\n",
    "x_cat = (x_residual + x_lstm) / 2\n",
    "combined = Dropout(p2)(x_cat)\n",
    "x_residual = Dense(m1*2)(combined)\n",
    "# combined = tf.transpose(combined,perm=[0,2,1])\n",
    "x_lstm = tf.keras.layers.Bidirectional(GRU(m2,return_state=True,return_sequences=True))(combined,initial_state=hidden)#2\n",
    "hidden=x_lstm[1:]\n",
    "x_lstm=x_lstm[0]\n",
    "# x_lstm = tf.transpose(x_lstm,perm=[0,2,1])\n",
    "x_cat = (x_residual + x_lstm) / 2\n",
    "combined = Dropout(p2)(x_cat)\n",
    "combined = GlobalAveragePooling1D()(combined)\n",
    "# combined = Flatten()(combined)\n",
    "# combined = Dense(m1*2, activation=tf.nn.leaky_relu)(combined)\n",
    "# v = Dense(5, activation=\"softmax\")(combined)\n",
    "combined = Model(inputs = [model_A.input, model_C.input] , outputs= combined)\n",
    "# model1 = Model(inputs = [model_A.input, model_C.input] , outputs= v)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# y = Conv1D(3,10, strides=10,activation=tf.nn.leaky_relu)(inputB)\n",
    "# y = Conv1D(2,5, strides =5, activation=tf.nn.leaky_relu)(y)\n",
    "# y = Dropout(0.25)(y)\n",
    "# y = Conv1D(1,2 ,strides=2 ,activation=tf.nn.leaky_relu)(y)\n",
    "# y = Dropout(0.25)(y)\n",
    "# y = Flatten()(y)\n",
    "# \"\"\"\n",
    "# y = Dense(100, activation=tf.nn.leaky_relu)(y)\n",
    "# y= Dense(50, activation=tf.nn.leaky_relu)(y)\n",
    "# y= Dense(5, activation=\"linear\")(y)\"\"\"\n",
    "\n",
    "# y = Model(inputs=inputB, outputs=y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "raws = BatchNormalization()(inputH)\n",
    "raws = Conv1D(filters=60, kernel_size=20, padding='SAME', activation=tf.nn.leaky_relu)(raws)\n",
    "raws = MaxPooling1D(pool_size=6)(raws)\n",
    "raws = Conv1D(filters=80, kernel_size= 15, padding='SAME', activation=tf.nn.leaky_relu)(raws)\n",
    "raws = MaxPooling1D(pool_size=6)(raws)\n",
    "raws = Dropout(0.25)(raws)\n",
    "raws = Conv1D(filters=80, kernel_size= 10, padding='SAME', activation=tf.nn.leaky_relu)(raws)\n",
    "raws = MaxPooling1D(pool_size=6)(raws)\n",
    "raws = Conv1D(filters=80, kernel_size= 10, padding='SAME', activation=tf.nn.leaky_relu)(raws)\n",
    "raws = MaxPooling1D(pool_size=4)(raws)\n",
    "raws = Dense(128, activation=tf.nn.leaky_relu)(raws)\n",
    "z = GlobalAveragePooling1D()(raws)\n",
    "    \n",
    "# z = Conv1D(128, kernel_size=7, strides=2, padding='SAME')(inputH)\n",
    "# z = LeakyReLU(alpha=0.1)(z)\n",
    "# z = Conv1D(128, kernel_size=7, strides=2, padding='SAME')(z)\n",
    "# z = LeakyReLU(alpha=0.1)(z)\n",
    "# z = Dropout(0.25)(z)\n",
    "# z = Conv1D(128, kernel_size=7, strides=2, padding='SAME')(z)\n",
    "# z = LeakyReLU(alpha=0.1)(z)\n",
    "# z = MaxPool1D(4)(z)\n",
    "# z = Conv1D(256, kernel_size=7, strides=2, padding='SAME')(z)\n",
    "# z = Conv1D(128, kernel_size=7, strides=2, padding='SAME')(z)\n",
    "# z = LeakyReLU(alpha=0.1)(z)\n",
    "# z = Flatten()(z)\n",
    "z = Model(inputs=inputH, outputs=z)\n",
    "\n",
    "last_layer =  concatenate([combined.output,z.output])\n",
    "#     z = Dense(200, activation=tf.nn.leaky_relu)(last_layer)\n",
    "\n",
    "#     z = Dense(150, activation=tf.nn.leaky_relu)(z)\n",
    "#     z= Dropout(0.25)(z)\n",
    "# v = Dense(25, activation=tf.nn.leaky_relu)(last_layer)\n",
    "v = Dense(5, activation=\"softmax\")(last_layer)\n",
    "\n",
    "model1 = Model(inputs=[model_A.input, model_C.input,z.input], outputs=v)\n",
    "print(model1.summary())\n",
    "# model2 = Model(inputs=[model_A.input, y.input, model_C.input], outputs=z)\n",
    "# model3 = Model(inputs=[model_A.input, y.input, model_C.input], outputs=z)\n",
    "\n",
    "\n",
    "# #######################################################################\n",
    "# # both the temporal and epoch needs to be reshape as follow and Concatenate over the -1 axis\n",
    "# # to have (batch_size, k.shape[1]+6,2) shape.\n",
    "#     print('k',k.shape)\n",
    "#     k=tf.keras.layers.Reshape((k.shape[1],-1))(k)\n",
    "#     print('k',k.shape)\n",
    "# #     k = tf.transpose(k,perm=[0,2,1])\n",
    "# ################################################################\n",
    "# #2-SkipGRU\n",
    "#     x_residual = Dense(units*2)(k)\n",
    "# #     k = tf.transpose(k,perm=[0,2,1])\n",
    "#     x_lstm = tf.keras.layers.Bidirectional(GRU(units,return_state=True,return_sequences=True))(k,initial_state=hidden)#1\n",
    "#     hidden=x_lstm[1:]\n",
    "#     x_lstm=x_lstm[0]\n",
    "# #     x_lstm = tf.transpose(x_lstm,perm=[0,2,1])\n",
    "#     x_cat = (x_residual + x_lstm) / 2\n",
    "#     k = Dropout(0.25)(x_cat)\n",
    "#     x_residual = Dense(units*2)(k)\n",
    "# #     k = tf.transpose(k,perm=[0,2,1])\n",
    "#     x_lstm = tf.keras.layers.Bidirectional(GRU(units,return_state=True,return_sequences=True))(k,initial_state=hidden)#2\n",
    "#     hidden=x_lstm[1:]\n",
    "#     x_lstm=x_lstm[0]\n",
    "# #     x_lstm = tf.transpose(x_lstm,perm=[0,2,1])\n",
    "#     x_cat = (x_residual + x_lstm) / 2\n",
    "#     k = Dropout(0.25)(x_cat)\n",
    "#     k = Flatten()(k)\n",
    "#     k= Dense(100,activation=tf.nn.leaky_relu)(k)\n",
    "#     k= Dense(5, activation='softmax')(k)\n",
    "\n",
    "#     model.summary()\n",
    "\n",
    "\n",
    "#model = Model(inputs=inputA, outputs=k)\n",
    "#ompile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_kfold = custom_Kfold(30)\n",
    "# \n",
    "for dic in dic_kfold: \n",
    "    train , test = dic.values()\n",
    "# for train, test in kfold.split(x_eeg, y_data):\n",
    "    if fold_no == 16:\n",
    "        print(test.shape)\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        model1.compile(loss=custom_loss,\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=['accuracy'])\n",
    "#         history = model1.fit([x_eeg, x_meta,x_eeg_normalized], y_data,batch_size=8,\n",
    "#                   epochs=50,verbose=1)\n",
    "        history = model1.fit([x_eeg[train], x_meta[train],x_eeg_normalized[train]], y_data[train],\n",
    "                 batch_size=32, \n",
    "                             validation_data=([x_eeg[test], x_meta[test],x_eeg_normalized[test]], y_data[test])\n",
    "                             ,callbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience = 25)],\n",
    "                  epochs=200,verbose=1)\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'DONE training for fold {fold_no} ...')\n",
    "#     # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.read_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "winsound.Beep(1000, 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model = load_model('./model.h5',compile=True)\n",
    "# del x_train\n",
    "# del x_validation\n",
    "\n",
    "def test_data(input_signals_list):\n",
    "    with h5py.File(file_xtest, \"r\") as fi:\n",
    "        index = fi['index_absolute'][()]\n",
    "        metadata =  fi[\"index_window\"][()]\n",
    "        if len(input_signals_list) == 1:\n",
    "            x_data = fi[input_signals_list[0]][()]\n",
    "        else:\n",
    "            x_data = np.zeros([24980,1500,len(input_signals_list)], dtype=np.float64)\n",
    "            for i in range(0, len(input_signals_list)):\n",
    "                if 'x' in input_signals_list[i] or 'y' in input_signals_list[i] or 'z' in input_signals_list[i]:\n",
    "                    f1 = interp1d(np.arange(0, 300), fi[input_signals_list[i]][()], axis=1)\n",
    "                    xnew = np.linspace(0, 30, num=1500)\n",
    "                    x_data[0:24980, 0:1500, i] = f1(xnew)\n",
    "                else:\n",
    "                    x_data[0:24980, 0:1500, i] = fi[input_signals_list[i]][()]\n",
    "        \n",
    "\n",
    "    # EEG extraction\n",
    "    mask_eeg = [0,1,2,3]\n",
    "\n",
    "    # You can't do spoc with K fold, or at least not here\n",
    "    # x_train_spoc, x_validation_spoc, spoc = SPoC_preprocessing(x_train[:,:,mask_eeg], y_train, x_validation[:,:,mask_eeg], 4)\n",
    "\n",
    "#     x_meta = positional_embeding(metadata)\n",
    "    x_eeg_normalized = normalize_data(x_data[:,:,mask_eeg])\n",
    "    x_eeg = np.copy(x_eeg_normalized)\n",
    "    x_eeg ,mean_eeg, var_eeg = stft_preprocessing(x_eeg)\n",
    "    x_meta = positional_embeding(metadata)\n",
    "    x_meta = np.append(x_meta[0:-1,:],x_meta[1:,:],axis=1)\n",
    "    print(x_meta.shape)\n",
    "    x_meta=x_meta.reshape(24979,2,6)\n",
    "    print(x_meta.shape)\n",
    "    x_eeg = np.append(x_eeg[0:-1,:,:,:],x_eeg[1:,:,:,:],axis=1)\n",
    "    print(x_eeg.shape)\n",
    "    # x_data.shape\n",
    "    x_eeg = x_eeg.reshape(24979,2,4,25,65)\n",
    "    print(x_eeg.shape)\n",
    "#         print(x_eeg_normalized.shape)\n",
    "    x_eeg_normalized = np.append(x_eeg_normalized[0:-1,:,:],x_eeg_normalized[1:,:,:],axis=1)\n",
    "    print(x_eeg_normalized.shape)\n",
    "\n",
    "    # X Y Z\n",
    "\n",
    "    if 'x' in input_signals_list :\n",
    "        mask_mvt= np.argwhere(np.logical_or(np.logical_or(input_signals_list == \"x\", input_signals_list == \"y\" ) , input_signals_list == \"z\" ))\n",
    "#             print(mask_mvt)\n",
    "        x_mvt, mean_mvt , var_mvt = coordinate_preprocessing(x_data[:,:,mask_mvt])\n",
    "#             print(x_mvt.shape)\n",
    "        x_mvt = np.squeeze(x_mvt)\n",
    "#             print(x_mvt.shape)\n",
    "\n",
    "\n",
    "    # index\n",
    "#     x_meta = positional_embeding(metadata)\n",
    "\n",
    "    print('data ready')\n",
    "\n",
    "    return x_eeg_normalized,x_eeg, x_meta ,index\n",
    "x_eeg_normalized_test,x_eeg_test, x_meta_test ,index = test_data(input_signals_list)\n",
    "\n",
    "y_test = model1.predict([x_eeg_test, x_meta_test,x_eeg_normalized_test])\n",
    "y_test = np.argmax(y_test,axis=1)\n",
    "y_test=np.append(y_test[0],y_test)\n",
    "if (index.shape) == (y_test.shape):\n",
    "    df = pd.DataFrame(data={'index': index, 'sleep_stage': y_test})\n",
    "    df.to_csv('model_num_'+str(fold_no)+'_our_result_SkipGRU_2nd.csv',index=False)\n",
    "    print('you can save the results')\n",
    "else:\n",
    "    print('there is an error in the shape of y_test')\n",
    "# for fold_no in range(1,4):\n",
    "#     model.load_weights(\"model_\"+str(fold_no)+\".hdf5\")\n",
    "#     y_test = model.predict_classes(Zxx_test)\n",
    "#     # df = pd.DataFrame(data={'index': index, 'sleep_stage': y_test})\n",
    "#     y_test=np.append(y_test[0],y_test)\n",
    "#     if (index.shape) == (y_test.shape):\n",
    "#         df = pd.DataFrame(data={'index': index, 'sleep_stage': y_test})\n",
    "#         df.to_csv('model_num_'+str(fold_no)+'_our_result_GRU.csv',index=False)\n",
    "#         print('you can save the results')\n",
    "#     else:\n",
    "#         print('there is an error in the shape of y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(data, row_labels, col_labels, ax=None,\n",
    "            cbar_kw={}, cbarlabel=\"\", **kwargs):\n",
    "    \"\"\"\n",
    "    Create a heatmap from a numpy array and two lists of labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        A 2D numpy array of shape (N, M).\n",
    "    row_labels\n",
    "        A list or array of length N with the labels for the rows.\n",
    "    col_labels\n",
    "        A list or array of length M with the labels for the columns.\n",
    "    ax\n",
    "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
    "        not provided, use current axes or create a new one.  Optional.\n",
    "    cbar_kw\n",
    "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
    "    cbarlabel\n",
    "        The label for the colorbar.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to `imshow`.\n",
    "    \"\"\"\n",
    "\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    # ... and label them with the respective list entries.\n",
    "    ax.set_xticklabels(col_labels)\n",
    "    ax.set_yticklabels(row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    for edge, spine in ax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im, cbar\n",
    "\n",
    "\n",
    "def annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\",\n",
    "                     textcolors=[\"black\", \"white\"],\n",
    "                     threshold=None, **textkw):\n",
    "    \"\"\"\n",
    "    A function to annotate a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im\n",
    "        The AxesImage to be labeled.\n",
    "    data\n",
    "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
    "    valfmt\n",
    "        The format of the annotations inside the heatmap.  This should either\n",
    "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
    "        `matplotlib.ticker.Formatter`.  Optional.\n",
    "    textcolors\n",
    "        A list or array of two color specifications.  The first is used for\n",
    "        values below a threshold, the second for those above.  Optional.\n",
    "    threshold\n",
    "        Value in data units according to which the colors from textcolors are\n",
    "        applied.  If None (the default) uses the middle of the colormap as\n",
    "        separation.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to each call to `text` used to create\n",
    "        the text labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(data, (list, np.ndarray)):\n",
    "        data = im.get_array()\n",
    "\n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max())/2.\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\",\n",
    "              verticalalignment=\"center\")\n",
    "    kw.update(textkw)\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "\n",
    "    return texts\n",
    "\n",
    "def plot_heatmap(y_true,y_pred): \n",
    "    \n",
    "    mat = confusion_matrix(y_true,y_pred)\n",
    "    mat = mat/np.sum(mat ,axis =0)*100\n",
    "    mat = mat.astype(np.int)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    im, cbar = heatmap(mat, ['Wake','NREM1','NREM2','NREM3','REM'],['Wake','NREM1','NREM2','NREM3','REM'], ax=ax,\n",
    "                       cmap=\"YlGn\", cbarlabel=\"Pourcentage de classification\")\n",
    "    texts = annotate_heatmap(im, valfmt=\"{x:} %\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "y_estimate = model1.predict([x_eeg[test], x_meta[test]])\n",
    "y_estimate = np.argmax(y_estimate,axis=1)\n",
    "# y_estimate=np.append(y_estimate[0],y_estimate)\n",
    "plot_heatmap(y_data[test], y_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = tf.constant(np.arange(2, 32*65, dtype=np.int32), shape=[32, 65, 1])\n",
    "# x_out= tf.constant(np.arange(2, 32*65*100, dtype=np.int32), shape=[32, 65, 100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init = tf.initializers.GlorotUniform()\n",
    "# context_matrix = tf.Variable(init(shape=(25,50*2)),trainable=True)\n",
    "# context_bias = tf.Variable(init(shape=(1,25)),trainable=True)\n",
    "# context_vector = tf.Variable(init(shape=(25,1)),trainable=True)\n",
    "\n",
    "# x = tf.random.uniform(shape=[64,65,100])\n",
    "# batch_size, length, n_features = x.shape\n",
    "# x_att = tf.reshape(x,[-1, n_features])\n",
    "# print('x_att.shape', x_att.shape)\n",
    "# u = tf.linalg.matmul(x_att,tf.transpose(context_matrix,perm=[1,0])) + context_bias\n",
    "# u = tf.nn.tanh(u)\n",
    "# print('u.shape',u.shape)\n",
    "# uv = tf.linalg.matmul(u,context_vector)\n",
    "# print('uv.shape',uv.shape)\n",
    "# uv = tf.reshape(uv, [-1, length])\n",
    "# print('uv.shape',uv.shape)\n",
    "# alpha = tf.nn.softmax(uv,axis=1)\n",
    "# print('alpha.shape',alpha.shape)\n",
    "# alpha = tf.expand_dims(alpha, axis=-1)\n",
    "# print('alpha.shape',alpha.shape)\n",
    "# x_out = alpha * x\n",
    "# print('x_out.shape',x_out.shape)\n",
    "# x_out=tf.math.reduce_sum(x_out, axis=1)\n",
    "# print('x_out.shape',x_out.shape)\n",
    "# print(x_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
