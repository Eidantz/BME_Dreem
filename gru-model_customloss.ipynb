{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.models import Sequential\n# from mne.decoding import SPoC\nfrom tensorflow.keras.layers import GlobalAveragePooling1D, Attention, Dense, Flatten, Conv2D, MaxPooling2D, Conv1D,MaxPool2D,GRU, Dropout, MaxPool1D, ConvLSTM2D,MaxPooling1D, ReLU, LeakyReLU, RNN, SimpleRNN, GRU, LSTM, TimeDistributed\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.layers import BatchNormalization\nfrom sklearn.model_selection import KFold\nfrom scipy.interpolate import interp1d\nimport tensorflow as tf\nfrom scipy.signal import stft\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport h5py # Read and write HDF5 files from Python\nimport os\nimport matplotlib.pyplot as plt2\nimport matplotlib\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom sklearn.metrics import confusion_matrix\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\ndata_path = \"/kaggle/input/dreem-2-sleep-classification-challenge-2020/\"\nfile_xtrain = data_path + \"X_train.h5/X_train.h5\"\nfile_xtest = data_path + \"X_test.h5/X_test.h5\"\nfile_ytrain = data_path + \"y_train.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# training labels\n# # what does the h5 file contains ?\n# with h5py.File(file_xtrain, \"r\") as hf:\n#        print(list(hf.keys()))\n\n# # How to load data from h5? what is its shape and type?\n# with h5py.File(file_xtrain, \"r\") as hf:\n#        field = list(hf.keys())[0]\n#        x_data = hf[field][()]\n# type(x_data), x_data.shape\n\ndef normalize_data(eeg_array):\n    \"\"\"normalize signal between 0 and 1\"\"\"\n\n    normalized_array = np.clip(eeg_array, -150, 150)\n    normalized_array = normalized_array / 150\n\n    return normalized_array\n\ndef split_data(input_signals_list,validation_ratio=0.3):\n    with h5py.File(file_xtrain, \"r\") as fi:\n        if len(input_signals_list) == 1:\n            x_data = fi[input_signals_list[0]][()]\n        else:\n            x_data = np.zeros([24688,1500,len(input_signals_list)], dtype=np.float64)\n            for i in range(0, len(input_signals_list)):\n                if 'x' in input_signals_list[i] or 'y' in input_signals_list[i] or 'z' in input_signals_list[i]:\n                    f1 = interp1d(np.arange(0, 300), fi[input_signals_list[i]][()], axis=1)\n                    xnew = np.linspace(0, 30, num=1500)\n                    x_data[0:24688, 0:1500, i] = f1(xnew)\n                else:\n                    x_data[0:24688, 0:1500, i] = fi[input_signals_list[i]][()]\n        \n        y_data = pd.read_csv(file_ytrain)['sleep_stage'].to_numpy()\n        \n\n        \n        #x_data_to_connect = x_data[1:,:,:]\n#         x_data = np.concatenate((x_data[0:-2,:,:],x_data[1:-1,:,:],x_data[2:,:,:]),axis=1)\n#         for a in range(0,x_data.shape[0]-1):\n#             x_data_new=np.zeros([x_data.shape[0]-1,3000,x_data.shape[2]])\n#             x_data_new[a,:,:] = np.append(x_data[a,:,:],x_data[a+1,:,:],axis=0)\n#         x_data = x_data_new\n#         x_data_new = 0\n        x_data = np.append(x_data[0:-1,:,:],x_data[1:,:,:],axis=1)\n#        x_metadata = x_metadata[1:]\n#        print(x_metadata.shape)\n#        y_data =np.delete(y_data,0)\n        # Creating data indices for training and validation splits:\n#        dataset_size = len(y_data)\n#        indices = list(range(dataset_size))\n#        split = int((1 - validation_ratio) * dataset_size)\n#        np.random.shuffle(indices)\n\n        \n        \n#        train_indices, val_indices = np.argwhere(x_metadata%5 != 0) ,np.argwhere(x_metadata%5 == 0)\n#        train_indices, val_indices =  np.squeeze(train_indices), np.squeeze(val_indices)\n\n\n#        x_train, x_validation = x_data[train_indices], x_data[val_indices]\n#        y_train, y_validation = y_data[train_indices], y_data[val_indices]\n\n        x_data =  normalize_data(x_data)\n        print('data ready')\n\n    return x_data ,y_data\ninput_signals_list = ['eeg_4','eeg_5']\n# input_signals_list = ['eeg_1','eeg_2','eeg_3','eeg_4','eeg_5','y']\n#input_signals_list = ['eeg_3','eeg_4','eeg_5']\n#input_signals_list = ['eeg_1','eeg_2','eeg_3','eeg_4','eeg_5','eeg_6','eeg_7']\n# input_signals_list = ['eeg_1','eeg_2','eeg_3','eeg_4','eeg_5','eeg_6','eeg_7','x','y','z']\n# input_signals_list = ['eeg_4','eeg_1']\n# input_signals_list = [ 'eeg_3', 'eeg_4', 'eeg_5']\ninputs, targets = split_data(input_signals_list)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_Kfold(n=5): \n    \n    with h5py.File(file_xtrain, \"r\") as fi:\n        x_metadata = fi['index'][()]\n        \n    \n    \n    indice = []\n    \n    for i in range(5): \n        train_indices, val_indices = np.argwhere(x_metadata%5 != 0+i) ,np.argwhere(x_metadata%5 == 0+i)\n\n\n        indice.append(  {\"train\":np.squeeze(train_indices), \"validation\": np.squeeze(val_indices)})\n    \n    return(indice)\n\n\n                      \n\n\n\n\n        \n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Model configuration\nbatch_size = 32\nloss_function = sparse_categorical_crossentropy\nno_classes = 5\nno_epochs = 20\noptimizer = Adam(learning_rate=0.001)\n# optimizer = SGD(learning_rate=0.01)\nverbosity = 1\nnum_folds = 3\n\n# Determine shape of the data\n\n# Define per-fold score containers\nacc_per_fold = []\nloss_per_fold = []\n\n# Merge inputs and targets\n# inputs = np.concatenate((x_train, x_validation), axis=0)\n# targets = np.concatenate((y_train, y_validation), axis=0)\n\n# Define the K-fold Cross Validator\n#kfold = KFold(n_splits=num_folds, shuffle=True)\n\n# K-fold Cross Validation model evaluation\nfold_no = 1\n\n# no_epochs = 80\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_accuracy') > 0.76):\n            if (logs.get('val_loss') < 0.6):\n                print(\"Reached 99% accuracy so cancelling trining!\")\n                self.model.stop_training = True\n\ncallbacks = myCallback()\n\ndef custom_loss(ytrue, ypred):\n    \n    \n    scce = tf.keras.losses.SparseCategoricalCrossentropy()\n    \n#     weight =tf.constant([[0.85347313, 0.93098802, 0.6152054,  0.79114608, 0.80918737]])\n    #weight =tf.constant([[0.00001, 1.0000, 0.00001, 0.00001, 0.001]])\n    weight = tf.constant([[0.94355903, 2.70683738, 0.96141876, 0.55233263, 0.62986756]])\n \n#     y_hot = tf.one_hot(ytrue, 5)\n    \n\n    \n    new_y = tf.expand_dims(ypred, axis=1)\n\n\n    new_weight = tf.matmul(weight,new_y, transpose_b = True)\n    \n    score = scce(ytrue,ypred, sample_weight= new_weight)\n    \n    return score\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model configuration\nbatch_size = 32\nloss_function = sparse_categorical_crossentropy\nno_classes = 5\nno_epochs = 20\noptimizer = Adam(learning_rate=0.001)\nverbosity = 1\nnum_folds = 3\nn_outputs=5\nn_features=len(input_signals_list)\nfold_no = 3\nno_epochs = 20\n# for train, test in kfold.split(inputs, targets):\n# n_steps, n_length = 2, 1500\n# trainX = inputs[train]\n# testX = inputs[test]\n# x_train, y_train, x_validation, y_validation\n# trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n# testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n# x_train = x_train.reshape((x_train.shape[0], n_steps, 1, n_length, n_features))\n# x_validation = x_validation.reshape((x_validation.shape[0], n_steps, 1, n_length, n_features))\n# Determine shape of the data\n\n# Define per-fold score containers\nacc_per_fold = []\nloss_per_fold = []\n\n# Merge inputs and targets\n#inputs = np.concatenate((x_train, x_validation), axis=0)\n#targets = np.concatenate((y_train, y_validation), axis=0)\n\n# Define the K-fold Cross Validator\n#kfold = KFold(n_splits=num_folds, shuffle=True)\n\n# K-fold Cross Validation model evaluation\nfold_no = 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" _, _, Zxx_train  = stft(inputs ,fs = 50, axis= 1)\n\nZxx_train = np.log(Zxx_train)\nZxx_train = np.clip(Zxx_train,-20,20)\n\nZxx_train = np.swapaxes(Zxx_train,1,2)\n# Merge inputs and targets\ninputs = Zxx_train\n\n\n# Define the K-fold Cross Validator\n\n# K-fold Cross Validation model evaluation\nfold_no = 1\n# checkpointer = [ModelCheckpoint(filepath= \"mixed_mod_simplesect_1draw_otherchannel.hdf5\",verbose=0, save_best_only=True), EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience = 10)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef custom_Kfold(n=5): \n    \n    with h5py.File(file_xtrain, \"r\") as fi:\n        x_metadata = fi['index'][()]\n    \n    x_metadata = x_metadata[1:]\n\n        \n    \n    \n    indice = []\n    \n    for i in range(5): \n        train_indices, val_indices = np.argwhere(x_metadata%5 != 0+i) ,np.argwhere(x_metadata%5 == 0+i)\n\n\n        indice.append(  {\"train\":np.squeeze(train_indices), \"validation\": np.squeeze(val_indices)})\n    \n    return(indice)\n\ndic_kfold = custom_Kfold()\n                      \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"number_channels = 2\n\nfor dic in dic_kfold: \n    train , test = dic.values()\n    \n    # Define the model architecture\n    model = Sequential()\n    model.add(tf.keras.layers.Reshape((number_channels, 25, 129),input_shape=(number_channels, 129, 25)))\n    # model.add(TimeDistributed(Dense(128,input_shape=(7, 129, 25))))\n    model.add(TimeDistributed(Dense(64)))\n    model.add(tf.keras.layers.Reshape((number_channels, 64, 25)))\n    model.add(tf.keras.layers.Reshape((25, 64, number_channels)))\n    model.add(TimeDistributed(Dense(1)))\n    model.add(tf.keras.layers.Reshape((1, 64, 25)))\n    model.add(Dropout(0.25))\n    model.add(tf.keras.layers.Reshape((1,25, 64)))\n    model.add(TimeDistributed(tf.keras.layers.Bidirectional(GRU(128, go_backwards=False))))\n#     model.add(ConvLSTM2D(filters=64, kernel_size=(1,7),padding='Same', return_sequences=True,input_shape=(n_steps, 1, n_length, n_features)))\n#     model.add(LeakyReLU(alpha=0.1))\n#     model.add(BatchNormalization())\n#     model.add(ConvLSTM2D(filters=128, kernel_size=(1,3),padding='Same', go_backwards=True))\n#     model.add(LeakyReLU(alpha=0.1))\n#     model.add(BatchNormalization())\n#     model.add(MaxPool2D(4,4))\n    model.add(Flatten())\n#     model.add(TimeDistributed(Flatten()))\n#     model.add(tf.keras.layers.Bidirectional(LSTM(128,return_sequences=True)))\n#     model.add(LeakyReLU(alpha=0.1))\n#     model.add(tf.keras.layers.Bidirectional(LSTM(128,return_sequences=True)))\n#     model.add(LeakyReLU(alpha=0.1))\n#     model.add(LSTM(64))\n#     model.add(LeakyReLU(alpha=0.1))\n    model.add(Dropout(0.2))\n    model.add(Dense(256))\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(Dense(100))\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(Dense(n_outputs, activation='softmax'))\n\n    # Compile the model\n    model.compile(loss=custom_loss,\n                optimizer=optimizer,\n                metrics=['accuracy'])\n\n    # Generate a print\n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {fold_no} ...')\n\n    # Fit data to model\n    history = model.fit(inputs[train], targets[train],\n              batch_size=batch_size, validation_data=(inputs[test], targets[test]),\n              epochs=200,callbacks = [ModelCheckpoint(filepath= \"model_\"+str(fold_no)+\".hdf5\",verbose=0, save_best_only=True),\n                                                     EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience = 10)],verbose=verbosity)\n    \n    # Generate generalization metrics\n    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n#     if (scores[0] < 0.57):\n#         model.save('model_num_'+str(fold_no)+'.h5')\n#         print(\"Saved model to disk\")\n    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n    acc_per_fold.append(scores[1] * 100)\n    loss_per_fold.append(scores[0])\n\n    # Increase fold number\n    fold_no = fold_no + 1\n\n# == Provide average scores ==\nprint('------------------------------------------------------------------------')\nprint('Score per fold')\nfor i in range(0, len(acc_per_fold)):\n  print('------------------------------------------------------------------------')\n  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\nprint('------------------------------------------------------------------------')\nprint('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\nprint(f'> Loss: {np.mean(loss_per_fold)}')\nprint('------------------------------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def heatmap(data, row_labels, col_labels, ax=None,\n            cbar_kw={}, cbarlabel=\"\", **kwargs):\n    \"\"\"\n    Create a heatmap from a numpy array and two lists of labels.\n\n    Parameters\n    ----------\n    data\n        A 2D numpy array of shape (N, M).\n    row_labels\n        A list or array of length N with the labels for the rows.\n    col_labels\n        A list or array of length M with the labels for the columns.\n    ax\n        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n        not provided, use current axes or create a new one.  Optional.\n    cbar_kw\n        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n    cbarlabel\n        The label for the colorbar.  Optional.\n    **kwargs\n        All other arguments are forwarded to `imshow`.\n    \"\"\"\n\n    if not ax:\n        ax = plt.gca()\n\n    # Plot the heatmap\n    im = ax.imshow(data, **kwargs)\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(data.shape[1]))\n    ax.set_yticks(np.arange(data.shape[0]))\n    # ... and label them with the respective list entries.\n    ax.set_xticklabels(col_labels)\n    ax.set_yticklabels(row_labels)\n\n    # Let the horizontal axes labeling appear on top.\n    ax.tick_params(top=True, bottom=False,\n                   labeltop=True, labelbottom=False)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Turn spines off and create white grid.\n    for edge, spine in ax.spines.items():\n        spine.set_visible(False)\n\n    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n    ax.tick_params(which=\"minor\", bottom=False, left=False)\n\n    return im, cbar\n\n\ndef annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\",\n                     textcolors=[\"black\", \"white\"],\n                     threshold=None, **textkw):\n    \"\"\"\n    A function to annotate a heatmap.\n\n    Parameters\n    ----------\n    im\n        The AxesImage to be labeled.\n    data\n        Data used to annotate.  If None, the image's data is used.  Optional.\n    valfmt\n        The format of the annotations inside the heatmap.  This should either\n        use the string format method, e.g. \"$ {x:.2f}\", or be a\n        `matplotlib.ticker.Formatter`.  Optional.\n    textcolors\n        A list or array of two color specifications.  The first is used for\n        values below a threshold, the second for those above.  Optional.\n    threshold\n        Value in data units according to which the colors from textcolors are\n        applied.  If None (the default) uses the middle of the colormap as\n        separation.  Optional.\n    **kwargs\n        All other arguments are forwarded to each call to `text` used to create\n        the text labels.\n    \"\"\"\n\n    if not isinstance(data, (list, np.ndarray)):\n        data = im.get_array()\n\n    # Normalize the threshold to the images color range.\n    if threshold is not None:\n        threshold = im.norm(threshold)\n    else:\n        threshold = im.norm(data.max())/2.\n\n    # Set default alignment to center, but allow it to be\n    # overwritten by textkw.\n    kw = dict(horizontalalignment=\"center\",\n              verticalalignment=\"center\")\n    kw.update(textkw)\n\n    # Get the formatter in case a string is supplied\n    if isinstance(valfmt, str):\n        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n\n    # Loop over the data and create a `Text` for each \"pixel\".\n    # Change the text's color depending on the data.\n    texts = []\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n            texts.append(text)\n\n    return texts\n\ndef plot_heatmap(y_true,y_pred): \n    \n    mat = confusion_matrix(y_true,y_pred)\n    mat = mat/np.sum(mat ,axis =0)*100\n    mat = mat.astype(np.int)\n\n    fig, ax = plt.subplots()\n\n    im, cbar = heatmap(mat, ['Wake','NREM1','NREM2','NREM3','REM'],['Wake','NREM1','NREM2','NREM3','REM'], ax=ax,\n                       cmap=\"YlGn\", cbarlabel=\"Pourcentage de classification\")\n    texts = annotate_heatmap(im, valfmt=\"{x:} %\")\n\n    fig.tight_layout()\n    plt.show()\n    \ny_estimate = model.predict_classes(Zxx_validation)\nplot_heatmap(y_validation, y_estimate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import winsound\nduration = 1000  # milliseconds\nfreq = 600  # Hz\nwinsound.Beep(freq, duration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.models import load_model\n# model = load_model('./model.h5',compile=True)\n# del x_train\n# del x_validation\nmodel.load_weights(\"model_1.hdf5\")\ndef test_data(input_signals_list):\n    with h5py.File(file_xtest, \"r\") as fi:\n        index = fi['index_absolute'][()]\n        if len(input_signals_list) == 1:\n            x_data = fi[input_signals_list[0]][()]\n        else:\n            x_data = np.zeros([24980,1500,len(input_signals_list)], dtype=np.float64)\n            for i in range(0, len(input_signals_list)):\n                if 'x' in input_signals_list[i] or 'y' in input_signals_list[i] or 'z' in input_signals_list[i]:\n                    f1 = interp1d(np.arange(0, 300), fi[input_signals_list[i]][()], axis=1)\n                    xnew = np.linspace(0, 30, num=1500)\n                    x_data[0:24980, 0:1500, i] = f1(xnew)\n                else:\n                    x_data[0:24980, 0:1500, i] = fi[input_signals_list[i]][()]\n        x_data = np.append(x_data[0:-1,:,:],x_data[1:,:,:],axis=1)\n        x_test = normalize_data(x_data)\n        return x_test, index\ntest_data,index = test_data(input_signals_list)\n_, _, Zxx_test  = stft(test_data ,fs = 50, axis= 1)\n\nZxx_test = np.log(Zxx_test)\nZxx_test = np.clip(Zxx_test,-20,20)\nZxx_test = np.swapaxes(Zxx_test,1,2)\ny_test = model.predict_classes(Zxx_test)\n# df = pd.DataFrame(data={'index': index, 'sleep_stage': y_test})\ny_test=np.append(y_test[0],y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if (index.shape) == (y_test.shape):\n    df = pd.DataFrame(data={'index': index, 'sleep_stage': y_test})\n    df.to_csv('our_result_GRU.csv',index=False)\n    print('you can save the results')\nelse:\n    print('there is an error in the shape of y_test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# for train, test in kfold.split(inputs, targets):\n\n#     # Define the model architecture\n#     model = Sequential()\n#     model.add(TimeDistributed(Dense(128,input_shape=(129, 7, 25))))\n#     model.add(TimeDistributed(tf.keras.layers.Bidirectional(GRU(128, go_backwards=False))))\n# #     model.add(ConvLSTM2D(filters=64, kernel_size=(1,7),padding='Same', return_sequences=True,input_shape=(n_steps, 1, n_length, n_features)))\n# #     model.add(LeakyReLU(alpha=0.1))\n# #     model.add(BatchNormalization())\n# #     model.add(ConvLSTM2D(filters=128, kernel_size=(1,3),padding='Same', go_backwards=True))\n# #     model.add(LeakyReLU(alpha=0.1))\n# #     model.add(BatchNormalization())\n# #     model.add(MaxPool2D(4,4))\n#     model.add(Flatten())\n# #     model.add(TimeDistributed(Flatten()))\n# #     model.add(tf.keras.layers.Bidirectional(LSTM(128,return_sequences=True)))\n# #     model.add(LeakyReLU(alpha=0.1))\n# #     model.add(tf.keras.layers.Bidirectional(LSTM(128,return_sequences=True)))\n# #     model.add(LeakyReLU(alpha=0.1))\n# #     model.add(LSTM(64))\n# #     model.add(LeakyReLU(alpha=0.1))\n#     model.add(Dropout(0.2))\n#     model.add(Dense(256))\n#     model.add(LeakyReLU(alpha=0.1))\n#     model.add(Dense(100))\n#     model.add(LeakyReLU(alpha=0.1))\n#     model.add(Dense(n_outputs, activation='softmax'))\n\n#     # Compile the model\n#     model.compile(loss=custom_loss,\n#                 optimizer=optimizer,\n#                 metrics=['accuracy'])\n\n#     # Generate a print\n#     print('------------------------------------------------------------------------')\n#     print(f'Training for fold {fold_no} ...')\n\n#     # Fit data to model\n#     history = model.fit(inputs[train], targets[train],\n#               batch_size=batch_size, validation_data=(inputs[test], targets[test]),\n#               epochs=200,callbacks = [ModelCheckpoint(filepath= \"model_\"+str(fold_no)+\".hdf5\",verbose=0, save_best_only=True),\n#                                                      EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience = 10)],verbose=verbosity)\n    \n#     # Generate generalization metrics\n#     scores = model.evaluate(inputs[test], targets[test], verbose=0)\n# #     if (scores[0] < 0.57):\n# #         model.save('model_num_'+str(fold_no)+'.h5')\n# #         print(\"Saved model to disk\")\n#     print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n#     acc_per_fold.append(scores[1] * 100)\n#     loss_per_fold.append(scores[0])\n\n#     # Increase fold number\n#     fold_no = fold_no + 1\n\n# # == Provide average scores ==\n# print('------------------------------------------------------------------------')\n# print('Score per fold')\n# for i in range(0, len(acc_per_fold)):\n#   print('------------------------------------------------------------------------')\n#   print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n# print('------------------------------------------------------------------------')\n# print('Average scores for all folds:')\n# print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n# print(f'> Loss: {np.mean(loss_per_fold)}')\n# print('------------------------------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}