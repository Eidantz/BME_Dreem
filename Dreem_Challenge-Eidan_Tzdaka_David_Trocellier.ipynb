{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep Stage Classification using EEG and IMU Data\n",
    "\n",
    "### Authors: Eidan Tzdaka, David Trocellier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py  # Read and write HDF5 files from Python\n",
    "import matplotlib.pyplot as plt  # For plotting graphs\n",
    "import numpy as np  # For numerical computations\n",
    "import pandas as pd  # For data processing (CSV, DataFrames)\n",
    "import tensorflow as tf  # TensorFlow for deep learning models\n",
    "from keras import Input, Model  # Model building components\n",
    "from scipy.signal import stft  # Short-time Fourier Transform for signal processing\n",
    "from sklearn.metrics import confusion_matrix, classification_report  # For evaluating model performance\n",
    "from sklearn.model_selection import KFold  # K-Fold cross-validation\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # Callback for stopping training early\n",
    "from tensorflow.keras.layers import (Lambda, BatchNormalization, MaxPooling1D, \n",
    "                                     Dense, Conv1D, Dropout, GRU, \n",
    "                                     TimeDistributed, GlobalAveragePooling1D)\n",
    "from tensorflow.keras.optimizers import Adam  # Optimizer for model training\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.getcwd()  # Get the current working directory\n",
    "file_xtrain = os.path.join(data_path, 'X_train.h5')  # Path to training data (EEG/IMU signals)\n",
    "file_xtest = os.path.join(data_path, 'X_test.h5')  # Path to testing data\n",
    "file_ytrain = os.path.join(data_path, 'y_train.csv')  # Path to labels for training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize EEG data between -150 and 150\n",
    "def normalize_data(eeg_array):\n",
    "    \"\"\"Normalize signal between 0 and 1.\"\"\"\n",
    "    normalized_array = np.clip(eeg_array, -150, 150)\n",
    "    normalized_array = normalized_array / 150\n",
    "    return normalized_array\n",
    "\n",
    "# Function to apply Short-Time Fourier Transform (STFT) for data preprocessing\n",
    "def stft_preprocessing(data, mean=None, var=None):\n",
    "    \"\"\"Apply STFT and perform mean-variance normalization.\"\"\"\n",
    "    zxx = stft(data, fs=50, nperseg=128, nfft=128, noverlap=64, axis=1)[2]\n",
    "    zxx = np.log(np.abs(zxx))\n",
    "    cliped = np.clip(zxx, -20, 20)\n",
    "    cliped = np.swapaxes(cliped, 1, 2)\n",
    "    cliped = np.swapaxes(cliped, 2, 3)\n",
    "\n",
    "    newdata = cliped\n",
    "    print(newdata.shape)\n",
    "    \n",
    "    if mean is None:\n",
    "        mean = newdata.mean()\n",
    "    newdata = newdata - mean\n",
    "    \n",
    "    if var is None:\n",
    "        var = newdata.var()\n",
    "    newdata = newdata / var\n",
    "\n",
    "    return newdata, mean, var\n",
    "\n",
    "# Function to create positional embeddings\n",
    "def positional_embeding(data):\n",
    "    \"\"\"Compute embedded position using cosine functions.\"\"\"\n",
    "    data = np.array(data)\n",
    "    pos = np.zeros([data.shape[0], 6])\n",
    "    pos[:, 0] = data / 1200  # Normalize data\n",
    "\n",
    "    angle = [30, 60, 90, 120, 150]\n",
    "    for i in range(5):\n",
    "        pos[:, i + 1] = np.cos((data * np.pi) / angle[i])\n",
    "    \n",
    "    return pos\n",
    "\n",
    "# Function to prepare and split the data into trainable components\n",
    "def split_data(input_list):\n",
    "    \"\"\"Prepare training data and return EEG raw, STFT, and embedded position data.\"\"\"\n",
    "    with h5py.File(file_xtrain, \"r\") as fi:\n",
    "        if len(input_list) == 1:\n",
    "            x_data = fi[input_list[0]][()]\n",
    "        else:\n",
    "            x_data = np.zeros([24688, 1500, len(input_list)], dtype=np.float64)\n",
    "            for i in range(0, len(input_list)):\n",
    "                if 'x' in input_list[i] or 'y' in input_list[i] or 'z' in input_list[i]:\n",
    "                    f1 = interp1d(np.arange(0, 300), fi[input_list[i]][()], axis=1)\n",
    "                    xnew = np.linspace(0, 30, num=1500)\n",
    "                    x_data[0:24688, 0:1500, i] = f1(xnew)\n",
    "                else:\n",
    "                    x_data[0:24688, 0:1500, i] = fi[input_list[i]][()]\n",
    "        \n",
    "        # Reading metadata and labels\n",
    "        y_data_org = pd.read_csv(file_ytrain)['sleep_stage'].to_numpy()\n",
    "        metadata = fi[\"index_window\"][()]\n",
    "\n",
    "        # Masking EEG channels and normalizing\n",
    "        mask_eeg = [0, 1, 2, 3]\n",
    "        x_eeg_raw_data = normalize_data(x_data[:, :, mask_eeg])\n",
    "        x_stft_data = np.copy(x_eeg_raw_data)\n",
    "\n",
    "        # Drop some channels to avoid overfitting\n",
    "        for subjects in range(x_stft_data.shape[0]):\n",
    "            for channels in range(x_stft_data.shape[2]):\n",
    "                if np.random.uniform() < 0.1:\n",
    "                    x_stft_data[subjects, :, channels] *= 0\n",
    "\n",
    "        # Perform STFT preprocessing\n",
    "        x_stft_data, mean_eeg, var_eeg = stft_preprocessing(x_stft_data)\n",
    "        \n",
    "        # Generate positional embeddings\n",
    "        x_emb_pos_data = positional_embeding(metadata)\n",
    "        x_emb_pos_data = np.append(x_emb_pos_data[0:-1, :], x_emb_pos_data[1:, :], axis=1)\n",
    "        x_emb_pos_data = x_emb_pos_data.reshape(24687, 2, 6)\n",
    "        \n",
    "        # Reshape STFT data\n",
    "        x_stft_data = np.append(x_stft_data[0:-1, :, :, :], x_stft_data[1:, :, :, :], axis=1)\n",
    "        x_stft_data = x_stft_data.reshape(24687, 2, 4, 25, 65)\n",
    "        \n",
    "        # Update EEG raw data\n",
    "        x_eeg_raw_data = np.append(x_eeg_raw_data[0:-1, :, :], x_eeg_raw_data[1:, :, :], axis=1)\n",
    "\n",
    "        y_data_org = y_data_org[1:]\n",
    "\n",
    "        print('Data preparation complete.')\n",
    "    \n",
    "    return x_eeg_raw_data, x_stft_data, x_emb_pos_data, y_data_org\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Preprocessing the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input signal list for training\n",
    "input_signals_list = ['eeg_4', 'eeg_5', 'eeg_6', 'eeg_7', 'x', 'y', 'z']\n",
    "\n",
    "# Prepare training data\n",
    "x_eeg_raw, x_stft, x_emb_pos, y_data = split_data(input_signals_list)\n",
    "\n",
    "# Check data shapes to ensure they are consistent\n",
    "print('y_data.shape = ', y_data.shape)\n",
    "print('x_eeg.shape = ', x_stft.shape)\n",
    "print('x_eeg_normalized.shape = ', x_eeg_raw.shape)\n",
    "print('x_meta.shape = ', x_emb_pos.shape)\n",
    "\n",
    "shapes = np.array([y_data.shape[0], x_stft.shape[0], x_eeg_raw.shape[0], x_emb_pos.shape[0]])\n",
    "if np.sum(np.abs(np.diff(shapes))) != 0:\n",
    "    print(\"Shape error in data alignment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function\n",
    "def custom_loss(ytrue, ypred):\n",
    "    \"\"\"SparseCategoricalCrossentropy with class weights.\"\"\"\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    weight = tf.constant([[0.94355903, 2.70683738, 0.96141876, 0.55233263, 0.62986756]])\n",
    "    \n",
    "    new_y = tf.expand_dims(ypred, axis=1)\n",
    "    new_weight = tf.matmul(weight, new_y, transpose_b=True)\n",
    "    score = scce(ytrue, ypred, sample_weight=new_weight)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer setup\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "# Cross-validation setup\n",
    "kfold = KFold(n_splits=30, shuffle=True)\n",
    "\n",
    "# Attention Layer class\n",
    "class Attention(Layer):\n",
    "    \"\"\"Custom attention layer based on Dreem open repository.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, context_size=25):\n",
    "        super(Attention, self).__init__()  \n",
    "        init = tf.initializers.GlorotUniform()\n",
    "        self.context_matrix = tf.Variable(init(shape=(context_size, input_dim)), trainable=True)\n",
    "        self.context_bias = tf.Variable(init(shape=(1, context_size)), trainable=True)\n",
    "        self.context_vector = tf.Variable(init(shape=(context_size, 1)), trainable=True)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'context_matrix': self.context_matrix,\n",
    "            'context_bias': self.context_bias,\n",
    "            'context_vector': self.context_vector\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        \"\"\"Compute the attention weights and apply them to the input.\"\"\"\n",
    "        batch_size, length, n_features = x.shape\n",
    "        x_att = tf.reshape(x, [-1, n_features])\n",
    "        u = tf.linalg.matmul(x_att, tf.transpose(self.context_matrix, perm=[1, 0])) + self.context_bias\n",
    "        u = tf.nn.tanh(u)\n",
    "        uv = tf.linalg.matmul(u, self.context_vector)\n",
    "        uv = tf.reshape(uv, [-1, length])\n",
    "        alpha = tf.nn.softmax(uv, axis=1)\n",
    "        alpha = tf.expand_dims(alpha, axis=-1)\n",
    "        x_out = alpha * x\n",
    "        x_out = tf.math.reduce_sum(x_out, axis=1)\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "\n",
    "# Function for creating K-fold validation for unique subjects\n",
    "def custom_Kfold(n):\n",
    "    \"\"\"Ensure training and validation happen on different subjects.\"\"\"\n",
    "    with h5py.File(file_xtrain, \"r\") as fi:\n",
    "        x_metadata = fi['index'][()]\n",
    "    \n",
    "    x_metadata = x_metadata[1:]\n",
    "    indices = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        train_indices, val_indices = np.argwhere(x_metadata % n != 0 + i), np.argwhere(x_metadata % n == 0 + i)\n",
    "        np.random.shuffle(train_indices)\n",
    "        np.random.shuffle(val_indices)\n",
    "        indices.append({\"train\": np.squeeze(train_indices), \"validation\": np.squeeze(val_indices)})\n",
    "\n",
    "    return indices\n",
    "\n",
    "# Reshaping function for backend usage\n",
    "def backend_reshape(x):\n",
    "    \"\"\"Reshape based on batch size and channels.\"\"\"\n",
    "    return tf.keras.backend.reshape(x, (-1, n_channels, spectrogram_length, filter_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hidden state for GRU and set fold number for cross-validation\n",
    "hidden = None  # Initial hidden state for GRU layers, will be updated during training\n",
    "fold_no = 1    # Counter to keep track of which fold is being processed in K-fold cross-validation\n",
    "\n",
    "# Model configuration parameters\n",
    "\n",
    "# Filter and channel reduction settings\n",
    "C = 4                # The number of channels to reduce to (channel reduction)\n",
    "filter_reduc = 40     # The number of filters to reduce in the first dense layer\n",
    "\n",
    "# Attention layer context size\n",
    "att_context = 20      # Size of the context for the attention mechanism\n",
    "\n",
    "# GRU (Gated Recurrent Unit) configuration\n",
    "m1 = 128  # Number of units in the first GRU layer\n",
    "m2 = 128  # Number of units in the second GRU layer\n",
    "\n",
    "# Dropout rates to prevent overfitting\n",
    "p1 = 0.2  # Dropout rate for the first GRU layer and channel reduction layers\n",
    "p2 = 0.2  # Dropout rate for the second GRU layer and other fully connected layers\n",
    "\n",
    "# Extract shape information from the input STFT data\n",
    "_, temporal_context, n_channels, spectrogram_length, filter_size = x_stft.shape  # Shape of the STFT data\n",
    "number_channels = n_channels  # Number of channels in the EEG data\n",
    "\n",
    "# Define inputs for the model\n",
    "inputA = Input(shape=(temporal_context, n_channels, spectrogram_length, filter_size))  # Input for STFT features\n",
    "inputC = Input(shape=(2, 6))  # Input for positional embeddings (metadata)\n",
    "inputD = Input(shape=134)  # Placeholder input, might be used for additional features (unused in this code)\n",
    "inputH = Input(shape=(3000, number_channels))  # Input for raw EEG data (used in the CNN)\n",
    "\n",
    "# Processing path for STFT data\n",
    "\n",
    "# Reshape inputA to adjust the dimensions for backend processing\n",
    "k = Lambda(backend_reshape)(inputA)\n",
    "\n",
    "# Apply filter reduction using TimeDistributed Dense layer\n",
    "k = TimeDistributed(Dense(filter_reduc))(k)  # Reduce the number of filters (40 in this case)\n",
    "k = tf.transpose(k, perm=[0, 3, 2, 1])  # Transpose to adjust the shape for the next operations\n",
    "\n",
    "# Apply channel reduction using TimeDistributed Dense layer\n",
    "k = TimeDistributed(Dense(C))(k)  # Reduce the number of channels (to 4 in this case)\n",
    "k = tf.transpose(k, perm=[0, 3, 2, 1])  # Transpose to maintain the correct shape\n",
    "\n",
    "# Apply dropout to reduce overfitting\n",
    "k = tf.keras.layers.Dropout(p1)(k)\n",
    "\n",
    "# Reshape and transpose for GRU layers\n",
    "k = tf.keras.layers.Reshape((-1, spectrogram_length))(k)  # Flatten for feeding into the GRU layer\n",
    "k = tf.transpose(k, perm=[0, 2, 1])  # Adjust dimensions for time steps\n",
    "\n",
    "# Apply bidirectional GRU layer with 128 units\n",
    "k = tf.keras.layers.Bidirectional(GRU(m1, return_sequences=True, go_backwards=False))(k)\n",
    "\n",
    "# Apply dropout again to prevent overfitting\n",
    "k = tf.keras.layers.Dropout(p1)(k)\n",
    "\n",
    "# Attention mechanism to focus on important features\n",
    "k = Attention(m1 * 2, att_context)(k)  # Apply attention with a context size of 20\n",
    "k = tf.reshape(k, [-1, temporal_context, 2 * m1])  # Reshape the output for further processing\n",
    "\n",
    "# Define the model for STFT data\n",
    "model_A = Model(inputs=inputA, outputs=k)\n",
    "\n",
    "# Processing path for positional embedding data (inputC)\n",
    "\n",
    "# Feed the positional embedding input through a series of dense layers\n",
    "m = Dense(6, activation=tf.nn.relu)(inputC)\n",
    "m = Dense(6, activation=tf.nn.relu)(m)\n",
    "m = Dense(6, activation=tf.nn.relu)(m)\n",
    "m = Dense(6, activation=tf.nn.relu)(m)\n",
    "m = Dense(6, activation=tf.nn.relu)(m)\n",
    "\n",
    "# Define the model for positional embedding\n",
    "model_C = Model(inputs=inputC, outputs=m)\n",
    "\n",
    "# Combining the outputs of model_A (STFT features) and model_C (positional embeddings)\n",
    "combined_input = concatenate([model_A.output, model_C.output])  # Concatenate the two outputs\n",
    "combined = tf.keras.layers.Reshape((combined_input.shape[1], -1))(combined_input)  # Reshape combined output\n",
    "\n",
    "# Apply a dense layer to the combined output for residual connection\n",
    "x_residual = Dense(m1 * 2)(combined)\n",
    "\n",
    "# Apply bidirectional GRU layer on the combined input with a residual connection\n",
    "x_lstm = tf.keras.layers.Bidirectional(GRU(m2, return_state=True, return_sequences=True))(combined,\n",
    "                                                                                          initial_state=hidden)  # First GRU\n",
    "hidden = x_lstm[1:]  # Update hidden state\n",
    "x_lstm = x_lstm[0]  # Extract GRU output\n",
    "x_cat = (x_residual + x_lstm) / 2  # Combine residual connection with GRU output\n",
    "combined = Dropout(p2)(x_cat)  # Apply dropout\n",
    "\n",
    "# Second layer with bidirectional GRU and residual connection\n",
    "x_residual = Dense(m1 * 2)(combined)\n",
    "x_lstm = tf.keras.layers.Bidirectional(GRU(m2, return_state=True, return_sequences=True))(combined,\n",
    "                                                                                          initial_state=hidden)  # Second GRU\n",
    "hidden = x_lstm[1:]  # Update hidden state\n",
    "x_lstm = x_lstm[0]\n",
    "x_cat = (x_residual + x_lstm) / 2  # Combine residual connection with GRU output\n",
    "combined = Dropout(p2)(x_cat)\n",
    "\n",
    "# Apply global average pooling to reduce the dimensionality\n",
    "combined = GlobalAveragePooling1D()(combined)\n",
    "\n",
    "# Define the combined model for STFT and positional embedding data\n",
    "combined = Model(inputs=[model_A.input, model_C.input], outputs=combined)\n",
    "\n",
    "# Processing path for raw EEG data using a 1D CNN\n",
    "\n",
    "# Normalize raw EEG data using batch normalization\n",
    "raws = BatchNormalization()(inputH)\n",
    "\n",
    "# Apply multiple Conv1D and MaxPooling1D layers to extract features from raw EEG data\n",
    "raws = Conv1D(filters=60, kernel_size=20, padding='SAME', activation=tf.nn.leaky_relu)(raws)\n",
    "raws = MaxPooling1D(pool_size=6)(raws)\n",
    "raws = Conv1D(filters=80, kernel_size=15, padding='SAME', activation=tf.nn.leaky_relu)(raws)\n",
    "raws = MaxPooling1D(pool_size=6)(raws)\n",
    "raws = Dropout(0.25)(raws)\n",
    "raws = Conv1D(filters=80, kernel_size=10, padding='SAME', activation=tf.nn.leaky_relu)(raws)\n",
    "raws = MaxPooling1D(pool_size=6)(raws)\n",
    "raws = Conv1D(filters=80, kernel_size=10, padding='SAME', activation=tf.nn.leaky_relu)(raws)\n",
    "raws = MaxPooling1D(pool_size=4)(raws)\n",
    "\n",
    "# Apply a dense layer to the feature-extracted data\n",
    "raws = Dense(128, activation=tf.nn.leaky_relu)(raws)\n",
    "\n",
    "# Apply global average pooling to reduce the dimensionality\n",
    "z = GlobalAveragePooling1D()(raws)\n",
    "\n",
    "# Define the model for raw EEG data\n",
    "z = Model(inputs=inputH, outputs=z)\n",
    "\n",
    "# Final model combining the outputs from the combined (STFT + positional embedding) and raw EEG CNN paths\n",
    "last_layer = concatenate([combined.output, z.output])\n",
    "\n",
    "# Apply a final dense layer with softmax activation for classification\n",
    "v = Dense(5, activation=\"softmax\")(last_layer)\n",
    "\n",
    "# Define the full model with all inputs and the final output\n",
    "model = Model(inputs=[model_A.input, model_C.input, z.input], outputs=v)\n",
    "\n",
    "# Print the model summary to view the architecture\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the history variable to store training information\n",
    "history = None\n",
    "\n",
    "# Generate subject-wise K-fold cross-validation splits\n",
    "dic_kfold = custom_Kfold(20)  # Create K-fold splits ensuring different subjects in train/validation sets\n",
    "\n",
    "# Run the model with K-fold cross-validation\n",
    "for dic in dic_kfold:\n",
    "    # Uncomment the line below if using K-fold splits manually\n",
    "    # train, test = dic.values()\n",
    "\n",
    "    # Check if it's the 15th fold for training\n",
    "    if fold_no == 15:\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "        # Compile the model with custom loss function and Adam optimizer\n",
    "        model.compile(loss=custom_loss,\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Train the model with the training data and validate on the test data\n",
    "        history = model.fit([x_stft[train], x_emb_pos[train], x_eeg_raw[train]], y_data[train],\n",
    "                            batch_size=32,  # Batch size for training\n",
    "                            validation_data=([x_stft[test], x_emb_pos[test], x_eeg_raw[test]], y_data[test]),  # Validation data\n",
    "                            callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=15)],  # Early stopping to prevent overfitting\n",
    "                            epochs=7,  # Train for 7 epochs\n",
    "                            verbose=1)  # Verbose output to track training progress\n",
    "\n",
    "        # Print completion message after training for the fold\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'DONE training for fold {fold_no} ...')\n",
    "\n",
    "    # Increase fold number for the next iteration\n",
    "    fold_no += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if history is not None:\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy with coustom loss')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss with coustom loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(data, row_labels, col_labels, ax=None,\n",
    "            cbar_kw={}, cbarlabel=\"\", **kwargs):\n",
    "    \"\"\"\n",
    "    Create a heatmap from a numpy array and two lists of labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        A 2D numpy array of shape (N, M).\n",
    "    row_labels\n",
    "        A list or array of length N with the labels for the rows.\n",
    "    col_labels\n",
    "        A list or array of length M with the labels for the columns.\n",
    "    ax\n",
    "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
    "        not provided, use current axes or create a new one.  Optional.\n",
    "    cbar_kw\n",
    "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
    "    cbarlabel\n",
    "        The label for the colorbar.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to `imshow`.\n",
    "    \"\"\"\n",
    "\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    # ... and label them with the respective list entries.\n",
    "    ax.set_xticklabels(col_labels)\n",
    "    ax.set_yticklabels(row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    for edge, spine in ax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im, cbar\n",
    "\n",
    "\n",
    "def annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\",\n",
    "                     textcolors=[\"black\", \"white\"],\n",
    "                     threshold=None, **textkw):\n",
    "    \"\"\"\n",
    "    A function to annotate a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im\n",
    "        The AxesImage to be labeled.\n",
    "    data\n",
    "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
    "    valfmt\n",
    "        The format of the annotations inside the heatmap.  This should either\n",
    "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
    "        `matplotlib.ticker.Formatter`.  Optional.\n",
    "    textcolors\n",
    "        A list or array of two color specifications.  The first is used for\n",
    "        values below a threshold, the second for those above.  Optional.\n",
    "    threshold\n",
    "        Value in data units according to which the colors from textcolors are\n",
    "        applied.  If None (the default) uses the middle of the colormap as\n",
    "        separation.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to each call to `text` used to create\n",
    "        the text labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(data, (list, np.ndarray)):\n",
    "        data = im.get_array()\n",
    "\n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max())/2.\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\",\n",
    "              verticalalignment=\"center\")\n",
    "    kw.update(textkw)\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "\n",
    "    return texts\n",
    "\n",
    "def plot_heatmap(y_true,y_pred): \n",
    "    \n",
    "    mat = confusion_matrix(y_true,y_pred)\n",
    "    mat = mat/np.sum(mat ,axis =0)*100\n",
    "    mat = mat.astype(np.int)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    im, cbar = heatmap(mat, ['Wake','NREM1','NREM2','NREM3','REM'],['Wake','NREM1','NREM2','NREM3','REM'], ax=ax,\n",
    "                       cmap=\"YlGn\", cbarlabel=\"Pourcentage de classification\")\n",
    "    texts = annotate_heatmap(im, valfmt=\"{x:} %\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "y_estimate = model.predict([x_stft[test], x_emb_pos[test], x_eeg_raw[test]])\n",
    "y_estimate = np.argmax(y_estimate, axis=1)\n",
    "print('confusion matrix with coustom loss')\n",
    "plot_heatmap(y_data[test], y_estimate)\n",
    "print('____________________________________________')\n",
    "print('')\n",
    "print('')\n",
    "print('classification report with coustom loss')\n",
    "print('')\n",
    "target_names=['Wake','NREM1','NREM2','NREM3','REM']\n",
    "# print(confusion_matrix(y_data[test], y_estimate))\n",
    "print(classification_report(y_data[test], y_estimate, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Predict the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(input_list):\n",
    "    '''get the test data the same as the traning data'''\n",
    "    with h5py.File(file_xtest, \"r\") as fi:\n",
    "        index = fi['index_absolute'][()]\n",
    "        metadata = fi[\"index_window\"][()]\n",
    "        if len(input_list) == 1:\n",
    "            x_data = fi[input_list[0]][()]\n",
    "        else:\n",
    "            x_data = np.zeros([24980, 1500, len(input_list)], dtype=np.float64)\n",
    "            for i in range(0, len(input_list)):\n",
    "                if 'x' in input_list[i] or 'y' in input_list[i] or 'z' in input_list[i]:\n",
    "                    f1 = interp1d(np.arange(0, 300), fi[input_list[i]][()], axis=1)\n",
    "                    xnew = np.linspace(0, 30, num=1500)\n",
    "                    x_data[0:24980, 0:1500, i] = f1(xnew)\n",
    "                else:\n",
    "                    x_data[0:24980, 0:1500, i] = fi[input_list[i]][()]\n",
    "\n",
    "    # EEG extraction\n",
    "    mask_eeg = [0, 1, 2, 3]\n",
    "\n",
    "    x_eeg_normalized = normalize_data(x_data[:, :, mask_eeg])\n",
    "    x_eeg = np.copy(x_eeg_normalized)\n",
    "    x_eeg, mean_eeg, var_eeg = stft_preprocessing(x_eeg)\n",
    "    x_meta = positional_embeding(metadata)\n",
    "    x_meta = np.append(x_meta[0:-1, :], x_meta[1:, :], axis=1)\n",
    "    x_meta = x_meta.reshape(24979, 2, 6)\n",
    "    x_eeg = np.append(x_eeg[0:-1, :, :, :], x_eeg[1:, :, :, :], axis=1)\n",
    "    x_eeg = x_eeg.reshape(24979, 2, 4, 25, 65)\n",
    "    x_eeg_normalized = np.append(x_eeg_normalized[0:-1, :, :], x_eeg_normalized[1:, :, :], axis=1)\n",
    "\n",
    "    print('data ready')\n",
    "\n",
    "    return x_eeg_normalized, x_eeg, x_meta, index\n",
    "\n",
    "\n",
    "x_eeg_raw_test, x_stft_test, x_emb_pos_test, index = test_data(input_signals_list)\n",
    "# predict the test data and save it as CSV\n",
    "y_test = model.predict([x_stft_test, x_emb_pos_test, x_eeg_raw_test])\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "y_test = np.append(y_test[0], y_test)\n",
    "if index.shape == y_test.shape:\n",
    "    df = pd.DataFrame(data={'index': index, 'sleep_stage': y_test})\n",
    "    df.to_csv('model_num_' + str(fold_no) + '_our_result_late.csv', index=False)\n",
    "    print('you can save the results')\n",
    "else:\n",
    "    print('there is an error in the shape of y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
